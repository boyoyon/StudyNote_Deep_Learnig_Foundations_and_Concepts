<!doctype html>
<html lang="ja">
    <head>
        <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
        <meta charset="utf-8" />
        <title>12章</title>
        <style type="text/css">
            p
            {
                padding-left: 2em;
            }
            .margin-large
            {
                margin-left: 30px;
            }
           .margin-abstract {
               margin-left: 60px; /* 左マージンを広くする */
               margin-right: 60px; /* 右マージンを広くする */
           }
        </style>
    <style>
        .two-columns {
            display: flex;
            flex-direction: row;
            gap: 20px; /* 列間のスペース */
        }
        .column {
            flex: 1; /* 各列が均等に幅を取る */
        }
    </style>
<style>
.three-columns {
  display: flex;
  gap: 10px; /* 列間の余白を設定 */
}
.column {
  flex: 1; /* 各列の幅を均等にする */
  padding: 10px; /* 内側の余白を設定 */
}
</style>
    <style>
        .styleRef { 
            text-indent: -40px; /* 最初の行の字下げを逆方向に */
            margin-left: 10px; /* 2行目以降の字下げを調整 */
            ul {
                  list-style-type: none; /* 箇条書き記号を非表示 */
                  padding-left: 40px; /* 全体の左余白 */
            }
            li {
            }
        }
    </style>
    <style>
        .styleBullet { 
            text-indent: -20px; /* 最初の行の字下げを逆方向に */
            margin-left: 30px; /* 2行目以降の字下げを調整 */
            ul {
                  list-style-type: none; /* 箇条書き記号を非表示 */
                  padding-left: 0px; /* 全体の左余白 */
            }
            li {
            }
        }
    </style>
    <style>
            ol
            {
                margin-left: 30px;
            }
            ul
            {
                margin-left: 30px;
            }
    </style>
    <style>
        .highlight {
            color: red; /* 好きな色に変更してください */
        }
    </style>
    </head>
    <body>
        <h1><center>12章 トランスフォーマー</center></h1>
<p>
トランスフォーマーは, 深層学習における最も重要な発展の1つです。これらは, アテンション(注意)と呼ばれる処理概念に基づいており, ネットワークが入力値に依存する重み係数を使用して, 異なる入力に異なる重みを与えることができるため, シーケンシャルなデータやその他の形式のデータに関連する強力な帰納バイアスを捕捉できます。
</p>
<p>
これらのモデルは, ある表現空間内のベクトルの集合を, 新しい空間内の同じ次元を持つ対応するベクトルの集合に変換するため, トランスフォーマー 変換器 として知られています。変換の目標は, 新しい空間が, 下流のタスクの解決により適した,
より豊富な内部表現を持つようになることです。トランスフォーマーへの入力は, 構造化されていないベクトルの集合, 順序付けられたシーケンス, またはより一般的な表現の形式を取ることができるため, トランスフォーマーに幅広い適用性が与えられます。
</p>
<p>
トランスフォーマーはもともと自然言語処理, つまりNLP 自然 言語とは英語や北京語などの言語です のコンテキストで導入され, リカレントニューラルネットワーク RNN に基づく以前の最先端のアプローチを大幅に上回りました。その後, トランスフォーマーが他の多くの分野で優れた結果を達成することが判明しました。たとえば, ビジョントランスフォーマーは, 画像処理タスクにおいてCNN よりも優れたパフォーマンスを発揮することがよくありますが, テキスト, 画像, オーディオ, ビデオなどの複数の種類のデータを組み合わせるマルチモーダルトランスフォーマーは, 最も強力な深層学習モデルの1 つです。
</p>
<p>
トランスフォーマーの主な利点の1 つは, 転移学習が非常に効果的であるため, トランスフォーマーモデルを大量のデータでトレーニングでき, トレーニングされたモデルを何らかの形式の微調整を使用して多くの下流タスクに適用できることです。 複数の異なるタスクを解決するために後で適合させることができる大規模モデルは, 基礎モデルとして知られています。さらに, トランスフォーマーは, ラベルなしのデータを使用して自己教師ありの方法でトレーニングできます。これは, トランスフォーマーがインターネットやその他のソースから入手可能な膨大な量のテキストを利用できるため, 言語モデルの場合に特に効果的です。スケーリング仮説は, 学習可能なパラメーターの数で測定されるモデルのスケールを大きくし, それに見合った大規模なデータセットでトレーニングするだけで, アーキテクチャを変更しなくても, パフォーマンスの大幅な向上が達成できると主張します。さらに, このトランスフォーマーは, グラフィックプロセッシングユニット GPU などの大規模並列処理ハードウェアに特に適しており, \(10 ^{12}\) 個のオーダーのパラメーターを持つ非常に大規模なニューラルネットワーク言語モデルを適切な時間内でトレーニングできるようになります。このようなモデルは並外れた能力を備えており, 汎用人工知能の初期の兆候として説明されている創発的な特性の明確な兆候を示します Bubeck et al., 2023 。
</p>
<p>
トランスフォーマーのアーキテクチャは, 複数の異なるコンポーネントが連携して動作し, さまざまな設計の選択が恣意的に行われるように見えるため, 初心者にとっては複雑で, 気が遠くなるようなものにさえ思えるかもしれません。したがって, この章では, トランスフォーマーの背後にあるすべての重要なアイデアを段階的に包括的に紹介し, さまざまな要素の設計を動機付けるための明確な直観を提供することを目的としています。最初にトランスフォーマーのアーキテクチャについて説明し,
次に自然言語処理に焦点を当ててから, 他のアプリケーションドメインを検討します。
</p>
<h2>12.1 アテンション</h2>
<p>
トランスフォーマーを支える基本的な概念は アテンションです。これは元々, 機械翻訳用の RNN の拡張機能として開発されました Bahdanau , Cho , およびBengio , 2014 。しかし, Vaswani ら 2017 は後に, 再帰構造を排除し, 代わりにアテンションメカニズムのみに焦点を当てることでパフォーマンスが大幅に向上することを示しました。現在, アテンションに基づくトランスフォーマーは, ほぼすべてのアプリケーションでRNN に完全に取って代わりました。
</p>
<p>
アテンションはより広範囲に適用可能ですが, 例として自然言語を使用してアテンションの利用を動機付けします。次の 2つの文を考えてみましょう。
</p>
<p class="margin^large">
● I swam across the river to get to the other <strong><span class="highlight">bank</span></strong>.<br>
　私は向こう<strong><span class="highlight">岸</span></strong>に行くために川を泳いで渡った。<br>
　<br>
● I walked across the road to get cash from the <strong><span class="highlight">bank</span></strong>.<br>
　私は<strong><span class="highlight">銀行</span></strong>から現金を受け取るために道路を渡って歩いた。
</p>
<p>
ここで, bank という単語は2 つの文で異なる意味を持っています。ただし, これはシーケンス内の他の単語によって提供されるコンテキストを調べることによってのみ検出できます。また, bank の解釈を決定する際に, 一部の単語が他の単語より重要であることもわかります。最初の文では, swam と river という単語が, bank が川の岸辺を指すことを最も強く示していますが,2
番目の文では, cash という単語が bank が金融機関を指すことを強く示しています。 bank の適切な解釈を決定するには, そのような文を処理するニューラルネットワークがシーケンスの残りの部分の特定の単語に注意を払う必要がある, つまり, より多くの単語に依存する必要があることがわかります。このアテンションの概念を図 12.1 に示します。
</p>
<center><img src="images/fig12_1.png"></center>
<p class="margin-large">
図12.1 。 bank という単語の解釈が river と swam という単語によって影響を受けるアテンションの概略図。各線の太さはその影響の強さを示しています。
</p>
<p>
さらに, より注目すべき特定の位置は入力シーケンス自体に依存していることもわかります。最初の文では重要なのは2 番目と5 番目の単語ですが, 2 番目の文では8 番目の単語です。標準的なニューラルネットワークでは, 異なる入力は, それらの入力を乗算する重みの値に応じて, 異なる範囲で出力に影響を与えます。ただし, ネットワークがトレーニングされると, それらの重みとそれに関連する入力は固定されます。対照的に, アテンションは, 値が特定の入力データに依存する重み係数を使用します。図 12.2 は, 自然言語でトレーニングされたトランスフォーマーネットワークのセクションからのアテンションの重みを示しています。
</p>
<center><img src="images/fig12_2.png"></center>
<p class="margin-large">
<center>図12.2 。学習されたアテンションの重みの例。 Vaswani ら 2017</center>
</p>
<p>
自然言語処理について説明する場合, 単語埋め込みを使用して単語を埋め込み空間内のベクトルにマッピングする方法がわかります。 これらのベクトルは, 後続のニューラル ネットワーク処理の入力として使用できます。 これらの埋め込みは, たとえば, 同様の意味を持つ単語を埋め込み空間内の近くの位置にマッピングすることによって, 基本的な意味論的特性を捕捉します。 このような埋め込みの特徴の1 つは, 特定の単語が常に同じ埋め込みベクトルにマップされることです。
</p>
<p>
トランスフォーマーは, 特定のベクトルがシーケンス内の他のベクトルに依存する位置にマッピングされる, よりリッチな形式の埋め込みとみなすことができます。したがって, 上記の例で bank を表すベクトルは, 2 つの異なる文の新しい埋め込み空間内の異なる場所にマッピングできます。たとえば, 最初の文では, 変換された表現により, 埋め込み空間内で bank が 水 に近くなる可能性がありますが, 2 番目の文では, 変換された表現により お金 に近くなる可能性があります。
</p>
<p>
アテンションの例として, タンパク質のモデリングを考えてみましょう。タンパク質は, アミノ酸と呼ばれる分子単位の一次元配列として見ることができます。タンパク質は潜在的に数百または数千のそのようなユニットを含むことができ, それぞれは22 の可能性のうちの1 つによって与えられます。生細胞では, タンパク質は3 次元構造に折りたたまれます。この構造では1 次元配列では大きく離れたアミノ酸が3 次元空間で物理的に近くなり, それによって相互作用することができます。トランスフォーマーモデルでは, これらの離れたアミノ酸が相互作用することができます。相互に 付随 することで, それらの3 次元構造をモデル化できる精度が大幅に向上します Vig et al., 2020 。
</p>
<h3>12.1.1. トランスフォーマー処理</h3>
<p>
トランスフォーマーへの入力データは, 次元 \(D\) の一連のベクトル \(\{\mathbf x_n\}\) です。ここで, \(n=1,..., N\) です。これらのデータベクトルをトークンと呼びます。ここで, トークンは, たとえば文内の単語, 画像内のパッチ, タンパク質内のアミノ酸などに対応します。トークンの要素 \(x_{n_i}\) は特徴と呼ばれます。 後で, 自然言語データおよび画像用のこれらのトークンベクトルを構築する方法を説明します。トランスフォーマーの強力な特性は, 異なるデータ型の混合を処理するために新しいニューラルネットワークアーキテクチャを設計する必要がなく, その代わりにデータ変数をトークンの結合セットに単純に結合できることです。
</p>
<p>
トランスフォーマーの動作を明確に理解する前に, 表記を正確にすることが重要です。標準的な規則に従って, データベクトルを次元 \(N×D\) の行列 \(X\) に結合します。この行列 \(X\) では, 図12.3 に図説するように \(n\) 番目の行がトークンベクトル \(\mathbf x_n^T\) で構成され \(n=1,...,N\) で行にラベルが付けられます。この行列は1 セットの入力トークンを表しており, ほとんどのアプリケーションでは, 各単語が1 つのトークンとして表されるテキストの独立したパッセージなど, 多くのトークンセットを含むデータセットが必要になることに注意してください。トランスフォーマーの基本的な構成要素は, データ行列を入力として受け取り, 出力と同じ次元の変換行列 \(X\) を作成する関数です。この関数は次の形式で書くことができます。
\[
\tilde{\mathbf X}=TransformaerLayer[\mathbf X] \tag{12.1}
\]
</p>
<p>
次に, 複数のトランスフォーマー層を連続して適用して, 豊富な内部表現を学習できる深いネットワークを構築できます。各トランスフォーマー層には独自の重みとバイアスが含まれており, この章で後ほど詳しく説明するように, 適切なコスト関数を使用した勾配降下法を使用して学習できます。
</p>
<center><img src="images/fig12_3.png"></center>
<p class="margin-large">
図12.3 。\(N×D\) 次元のデータ行列 \(X\) の構造。行 \(n\) は転置されたデータベクトル \(\mathbf x_n^T\) を表します。
</p>
<p>
単一のトランスフォーマー層自体は2 つのステージで構成されます。アテンションメカニズムを実装する第1 ステージでは, データ行列の列全体で異なるトークンベクトルからの対応する特徴を混合します。一方, 第2 ステージは各行に独立して作用し,
各トークンベクトル内の特徴を変換します。まず, アテンションのメカニズムを見てみましょう。
</p>
<h3>12.1.2 アテンション係数</h3>
<p>
埋め込み空間に入力トークンの集合 \(\mathbf x_1, \cdots, \mathbf x_N\) があり, これを同じ数のトークンを持つがより豊かな意味構造を捕らえる別の集合 \(\mathbf y_1, \cdots, \mathbf y_N\) にマッピングしたいとします。特定の出力ベクトル \(\mathbf y_n\) を考えてみましょう。\(\mathbf y_n\) の値は, 対応する入力ベクトル \(\mathbf x_n\) だけではなく, 集合内のすべてのベクトル \(\mathbf x_1, \cdots, \mathbf x_N\) にも依存する必要があります。 注意すると, この依存関係は, \(\mathbf y_n\) の修正された表現を決定するために特に重要な入力 \(\mathbf x_m\) に対してより強くなるはずです。これを実現する簡単な方法は, 各出力ベクトル \(\mathbf y_n\) を, 重み付け係数 \(a_{nm}\) を使用した入力ベクトル\(\mathbf x_1,\cdots, \mathbf x_N\) の線形結合になるように定義することです。
\[
\mathbf y_n=\sum_{m=1}^N a_{nm}\mathbf x_m \tag{12.2}
\]
ここで, \(a_{nm}\) はアテンション重みと呼ばれます。係数は, 出力 \(\mathbf y_n\) にほとんど影響を及ぼさない入力トークンではゼロに近く, 最も大きな影響を与える入力では最大になる必要があります。したがって, ある係数が大きく正になる一方で, 別の係数が大きく負になることで補うという状況を避けるために, 係数が負でないように制約します。また, 出力が特定の入力により多くの注意を払う場合, 他の入力への注意が低下することを犠牲にして, 係数の合計が1 になるように制約します。したがって,
重み付け係数は次の2 つの制約を満たす必要があります。
\[
\begin{align}
&a_nm \geq 0 \tag{12.3} \\
\\
&\sum_{m=1}^N a_{nm} =1 \tag{12.4}
\end{align}
\]
</p>
<p>
これらを総合すると, 各係数が \(0≦a_{nm}≦1\) の範囲内にあり, そのため係数が 1 の分割 を定義することを意味します。特殊なケース \(a_{mm}=1\) では, \(n≠m\) に対して \(a_{nm} = 0\) となり, したがって \(\mathbf y_m=\mathbf x_m\) となり, 入力ベクトルは変換によって変更されません。より一般的には, 出力 \(\mathbf y_m\) は, 一部の入力に他の入力よりも大きな重みが与えられた入力ベクトルのブレンドです。
</p>
<p>
各出力ベクトル \(\mathbf y_n\) に対して異なる係数のセットがあり, 制約 12.3 と 12.4 が \(n\) の値ごとに個別に適用されることに注意してください。これらの係数は入力データに依存しますが, その計算方法については後ほど説明します。
</p>
<h3>12.1.3 セルフアテンション</h3>
<p>
次の問題は, 係数 \(a_{nm}\) をどのように決定するかです。これについて詳しく説明する前に, まず情報検索の分野で使われている用語をいくつか紹介しておきます。オンライン映画ストリーミングサービスでどの映画を見るかを選択するという問題を考えてみましょう。1 つのアプローチは, 各映画を, ジャンル コメディ, アクションなど)), 主演俳優の名前, 映画の長さなどを説明する属性のリストに関連付けることです。ユーザーはカタログを検索して, 自分の好みに合った映画を見つけることができます。各ムービーの属性をキーと呼ばれるベクトルにエンコードすることで, これを自動化できます。対応する動画ファイル自体をバリューと呼びます。同様に, ユーザーは, 必要な属性に対して独自の値のベクトル クエリと呼ばれます を提供できます。その後, 映画サービスはクエリベクトルをすべてのキーベクトルと比較して最適な一致を見つけ, 対応する映画を値ファイルの形式でユーザーに送信できます。ユーザーが, キーがクエリに最も近い特定の映画に 参加している と考えることができます。これは, 単一の値ベクトルが返されるハードアテンションの一形態と考えられます。トランスフォーマーの場合, これをソフトアテンションに一般化し, 連続変数を使用してクエリとキー間の一致度を測定し, これらの変数を使用して出力に対する値ベクトルの影響を重み付けします。これにより, 変換関数が微分可能であることも保証され, 勾配降下法でトレーニングできるようになります。
</p>
<p>
情報検索との類似に従って, 各入力ベクトルxn を, 出力トークンの作成に使用される値ベクトルとして見ることができます。 また, ベクトルx n を入力トークンn のキー ベクトルとして直接使用します。それは, 映画そのものを使って映画の特徴を要約することに似ています。 最後に, \(\mathbf x_m\) を出力 \(\mathbf y_m\) のクエリ ベクトルとして使用し, 各キー ベクトルと比較できます。\(\mathbf x_n\) で表されるトークンが \(\mathbf x_m\) で表されるトークンにどの程度注意を払う必要があるかを確認するには, これらのベクトルがどの程度類似しているかを計算する必要があります。類似性の簡単な尺度の1 つは, それらの内積 \(\mathbf x_n^T\mathbf x_m\) を取得することです。制約12.3 と 12.4 を課すために, softmax 関数を使用してドット積を変換することにより, 重み付け係数 \(a_{nm}\) を定義できます。
\[
a_{nm}=\frac{exp(\mathbf x_n^T\mathbf x_m)}{\sum_{m^\prime=1}^N
 exp(\mathbf x_n^T\mathbf x_{m^\prime})} \tag{12.5}
\]
この場合, ソフトマックス関数の確率的な解釈はなく, 単にアテンションの重みを適切に正規化するために使用されていることに注意してください。
</p>
<p>
要約すると, 各入力ベクトル \(\mathbf x_n\) は, 12.2 の形式の入力ベクトルの線形結合を取ることによって, 対応する出力ベクトル \(\mathbf y_n\) に変換されます。入力ベクトル \(\mathbf x_m\) に適用される重み \(a_{nm}\) は, ソフトマックス関数 12.5 によって与えられます。入力 \(n\) のクエリ \(\mathbf x_n\) と入力 \(m\) に関連付けられたキー \(\mathbf x_m\) の間のドット積 \(\mathbf x_n^T\mathbf x_m\) で定義されます。すべての入力ベクトルが直交している場合, 各出力ベクトルは対応する入力ベクトルと単純に等しくなり \(m = 1, \cdots, N\) に対して \(\mathbf y_m = \mathbf x_m\) なることに注意してください。
</p>
<p>
データ行列 \(X\) と, 行が \(\mathbf y_m\) で与えられる類似の \(N×D\) 出力行列 \(Y\) を使用して 12.2 を行列表記で書くことができます。
\[
Y=Softmax[XX^T]X \tag{12.6}
\]
</p>
<p>
ここで, \(Softmax[L]\) は, 行列 \(L\) のすべての要素の指数を取得し, 各行を個別に正規化して合計が1 になる演算子です。ここからは, わかりやすくするために行列表記に焦点を当てていきます。
</p>
<p>
同じシーケンスを使用してクエリ, キー, 値を決定するため, このプロセスはセルフアテンションと呼ばれます。この注意メカニズムの変形については, この章の後半で説明します。また, クエリベクトルとキーベクトル間の類似性の尺度はドット積によって与えられるため, これはドット積セルフアテンションとして知られています。
</p>
<h3>12.1.4 ネットワーク・パラメータ</h3>
<p>
現状では, 入力ベクトル \(\{\mathbf x_n\}\) から出力ベクトル \(\{\mathbf y_n\}\) への変換は固定されており, 調整可能なパラメーターがないため, データから学習する能力がありません。さらに, トークンベクトル \(\mathbf x_n\) 内の各特徴値は, 注目係数を決定する際に同等の役割を果たしますが, ネットワークには, トークンの類似性を決定する際に, 他の特徴よりも一部の特徴に重点を置く柔軟性を持たせたいと考えています。元のベクトルの線形変換によって与えられる修正された特徴ベクトルを次の形式で定義すると, 両方の問題に対処できます。
\[
\tilde{X}=XU \tag{12.7}
\]
ここで, \(U\) は学習可能な重みパラメータの \(D×D\) 行列で, 標準的なニューラルネットワークの層に似ています。これにより, フォームの変更された変換が得られます。
\[
Y=Softmax[XUU^TX^T]XU \tag{12.8}
\]
</p>
<p>
これは柔軟性がはるかに優れていますが, マトリックスが次のような特性を持っています。
\[
XUU^TX^T \tag{12.8}
\]
これは対称ですが, アテンションメカニズムには重大な非対称性をサポートしてもらいたいと考えています。たとえば, すべてのノミは道具であるため, ノミ は ツール と強く関連付けられるはずですが, ノミ以外にも多くの種類のツールがあるため,
ツール は ノミ と弱く関連付けられるだけであると予想できます。ソフトマックス関数は, 結果として得られるアテンション重みの行列自体が対称ではないことを意味しますが, クエリとキーに独立したパラメーターを持たせることで, より柔軟なモデルを作成できます。さらに, 形式 12.8 は同じパラメータ行列U を使用して値ベクトルと注意係数の両方を定義しますが, これも望ましくない制限のように思えます。
</p>
<p>
これらの制限は, それぞれが独自の独立した線形変換を持つ個別のクエリ, キー, および値の行列を定義することで克服できます。
\[
\begin{align}
Q &= ZW^{(q)} \tag{12.10} \\
\\
K &= XW^{(k)} \tag{12.11} \\
\\
V &=XW^{(v)} \tag{12.12}
\end{align}
\]
ここで, 重み行列 \(W^{(q)}, W^{(k)}\), および \(W^{(v)}\) は, 最終的なトランスフォーマー・アーキテクチャのトレーニング中に学習されるパラメーターを表します。ここで, 行列 \(W^{(k)}\) の次元は \(D×D_k\) です。ここで, \(D_k\) はキー ベクトルの長さです。クエリベクトルとキーベクトルの間の内積を形成できるように, 行列 \(W^{(q)}\) は \(W^{(k)}\) と同じ次元 \(D×D_k\) を持たなければなりません。 一般的な選択は \(D_k = D\) です。同様に, \(W^{(v)}\) はサイズ \(D×D_v\) の行列で, \(D_v\) は出力ベクトルの次元を決定します。出力表現が入力と同じ次元になるように \(D_v = D\) を設定すると, 後で説明する残差接続の組み込みが容易になります。また, 各層の次元が同じであれば, 複数のトランスフォーマー層を相互に積み重ねることができます。次に, 12.6 を一般化して次のようにすることができます。
\[
Y=Softmax[QK^T]V \tag{12.13}
\]
ここで, \(QK^T\) の次元は \(N×N\) で, 行列 \(Y\) の次元は \(N×D_v\) です。行列 \(QK^T\) の計算を図12.4 に示し, 行列 \(Y\) の評価を図12.5 に示します。
</p>
<center><img src="images/fig12_4.png"></center>
<p class="margin-large">
図12.4 。トランスフォーマーのアテンション係数を決定する行列 \(QK^T\) の評価の図。入力 \(X\) は, 12.10 と 12.11 を使用して個別に変換され, それぞれクエリ行列 \(Q\) とキー行列 \(K\) が得られ, 次にそれらが乗算されます。
</p>
<center><img src="images/fig12_5.png"></center>
<p class="margin-large">
図12.5 。クエリ行列, キー行列, 値行列それぞれ \(Q, K, V\) を与えた場合のアテンション層からの出力の評価の図。出力行列Y で強調表示された位置のエントリは, \(Softmax[QK^T]\) 行列と \(V\) 行列の強調表示された行と列の内積からそれぞれ取得されます。
</p>
<p>
実際には, これらの線形変換にバイアスパラメーターを含めることもできます。ただし, 標準のニューラルネットワークで行ったように, データ行列X に1 の追加列を追加し, バイアスを表すパラメータの追加行で重み行列を拡張することにより, バイアスパラメータを重み行列に吸収することができます。今後は, 表記が煩雑になることを避けるために, バイアスパラメータを暗黙的なパラメータとして扱います。
</p>

<p>
従来のニューラルネットワークと比較して, 信号パスは活性化値間に乗算関係を持ちます。標準的なネットワークではアクティベーションに固定の重みを乗算しますが, ここではアクティベーションにデータ依存のアテンション係数を乗算します。これは, たとえば, 入力ベクトルの特定の選択に対して注目係数の1 つがゼロに近い場合, 結果の信号パスは対応する入力信号を無視するため, ネットワーク出力に影響を与えないことを意味します。対照的に, 標準的なニューラルネットワークが特定の入力または隠れユニット変数を無視することを学習すると, すべての入力ベクトルに対して無視します。
</p>
<h3>12.1.5 スケーリングされたセルフアテンション</h3>
<p>
セルフアテンション層に対して行うことができる最後の調整が1 つあります。ソフトマックス関数の勾配は, tanh 関数やロジスティック シグモイド活性化関数の場合と同様に, 入力の大きさが大きい場合には指数関数的に小さくなることを思い出してください。 これを防ぐには, ソフトマックス関数を適用する前に, クエリとキーベクトルの積を再スケーリングします。適切なスケーリングを導出するには, クエリベクトルとキーベクトルの要素がすべて, 平均と単位分散がゼロの独立した乱数である場合, 内積の分散は \(D_k\) になることに注意してください。したがって, \(D_k\) の平方根で与えられる標準偏差を使用して引数をソフトマックスに正規化し, 注目層の出力は次の形式になります。
\[
\mathbf Y=Attention(\mathbf Q,\mathbf K,\mathbf V)\equiv Softmax\left[\frac{\mathbf Q \mathbf K^T}{\sqrt{D_k}}\right]\mathbf V \tag{12.14}
\]
これはスケーリングされたドット積セルフ アテンションと呼ばれ, セルフアテンションニューラル ネットワーク層の最終形式です。この層の構造は, 図12.6 とアルゴリズム12.1 にまとめられています。
</p>
<p>
図12.6 。スケーリングされたドット積セルフアテンションニューラルネットワーク層の情報の流れ。ここで, mat mul は行列の乗算を示し, scale は \(D_k\) を使用したソフトマックスへの引数の正規化を示します。この構造は, 単一の注意 ヘッドを構成します。
</p>
<p>
\[
\boxed{
\begin{array}{l}
\textbf{アルゴリズム 12.1:　スケーリングされたドット積セルフアテンション} \\
\hline
入力 :トークンの集合 X\in \mathbb R^{N\times D}:\{\mathbf x_1,\cdots,\mathbf x_N\}\\
　　　重み行列 \{W^{(q)},W^{(k)}\}\in \mathbb R^{D\times D_k} と W^{(v)}\in \mathbb R^{D\times D_v}\\
出力：アテンション(Q,K,V)\in \mathbb R^{N\times D_v}:\{\mathbf y_1,\cdots,\mathbf y_N\} \\

\hline
Q=XW^{(q)}　// クエリ　Q\in\mathbb R^{N\times D_k}を計算する\\
K=XW^{(k)}　// キー　　K\in\mathbb R^{N\times D_k}を計算する\\
V=XW^{(v)}　// バリューV\in\mathbb R^{N\times D}を計算する\\
return　Attention(Q,K,V)=Softmax\left[\frac{QK^T}{\sqrt{D_k}}\right]V
\end{array}
}
\]
</p>
<h3>12.1.6 マルチヘッド・アテンション</h3>
<p>
これまで説明したアテンション層は, 出力ベクトルが入力ベクトルのデータ依存パターンに対応できるようにするものであり,
アテンションヘッドと呼ばれます。ただし, 同時に関連する注意のパターンが複数存在する可能性があります。たとえば, 自然言語では, 時制に関連するパターンもあれば, 語彙に関連するパターンもあります。単一のアテンションヘッドを使用すると, これらの効果が平均化される可能性があります。代わりに, 複数のアテンションヘッドを並行して使用できます。これらは, クエリ, キー, および値の行列の計算を制御する独立した学習可能なパラメーターを備えた, 単一ヘッドの同一構造のコピーで構成されます。これは, 畳み込みネットワークの各層で複数の異なるフィルターを使用することに似ています。
</p>
<p>
次の形式の \(h = 1,\cdots, H\) でインデックス付けされた \(H\) 個のヘッドがあるとします。
\[
H_h=Attention(Q_h,K_h,V_h) \tag{12.15}
\]
ここで, \(Attention( ･, ･, ･)\) は 12.14 で与えられ, 各ヘッドに対して個別のクエリ, キー, およびバリューの行列を次のように定義しました。
\[
\begin{align}
Q_h &= XW_h^{(q)} \tag{12.16} \\
\\
K_h &= XW_h^{(k)} \tag{12.17} \\
\\
V_h &= XW_h^{(v)} \tag{12.18}
\end{align}
\]
まずヘッドが1 つの行列に連結され, その結果が行列 \(W^{(o)}\) を使用して線形変換され, 次の形式で結合された出力が得られます。
\[
Y(X)=Concat[H_1,\cdots,H_H]W^{(o)} \tag{12.19}
\]
これを図 12.7 に示します。
</p>
<center><img src="images/fig12_7.png"></center>
<p>
図12.7 。マルチヘッドアテンションのためのネットワークアーキテクチャ。各ヘッドは図12.6 に示す構造で構成され, 独自のキー, クエリ, 値パラメータを持ちます。ヘッドの出力は連結され, 入力データの次元に線形投影されます。
</p>
<p>
各行列 \(H_h\) の次元は \(N×D_v\) であるため, 連結された行列の次元は\(N×HD_v\) になります。これは, 次元 \(HD_v×D\) の線形行列 \(W^{(o)}\) によって変換され, 元の入力行列 \(X\) と同じ次元 \(N×D\) の最終出力行列 \(Y\) が得られます。行列 \(W^{(o)}\) の要素 これらは, トレーニング フェーズ中にクエリ, キー, および値のマトリックスとともに学習されます。 通常, \(D_v\) は \(D/H\) に等しくなるように選択され, 結果の連結行列の次元が \(N×D\) になるようにします。マルチヘッド アテンションはアルゴリズム 12.2 にまとめられており, マルチヘッド アテンション層の情報フローは図 12.8 に示されています。
</p>
<p>
\[
\boxed{
\begin{array}{l}
\textbf{アルゴリズム 12.2:　マルチヘッド・アテンション} \\
\hline
入力 :トークンの集合 X\in \mathbb R^{N\times D}:\{\mathbf x_1,\cdots,\mathbf x_N\}\\
　　　クエリ重み行列 \{W_1^{(q)},\cdots,W_H^{(q)}\}\in \mathbb R^{D\times D}\\
　　　キー重み行列 \{W_1^{(k)},\cdots,W_H^{(k)}\}\in \mathbb R^{D\times D}\\
　　　バリュー重み行列 \{W_1^{(v)},\cdots,W_H^{(v)}\}\in \mathbb R^{D\times D}\\
　　　出力重み行列 W^{(o)}\in \mathbb R^{HD_v\times D}
 と W^{(v)}\in \mathbb R^{D\times D_v}\\
出力：Y \in \mathbb R^{N\times D}:\{\mathbf y_1,\cdots,\mathbf x_N\} \\
\hline
// 各ヘッドに対してセルフアテンションを計算する(アルゴリズム12.1)\\
for\;h=1,\cdots,H\;do\\
　|　Q_h ＝XW_h^{(q)},　K_h=XW_h^{(k)},　V_h=XW_h^{(v)} \\
　|　H_h = Attention(Q_h,K_h,V_h) // ヘッドを連結する \\
end\,for\\
H=Concat[H_1,\cdots,H_N] \\
return　Y(X)=HW^{(o)}
\end{array}
}
\]
</p>
<center><img src="images/fig12_8.png"></center>
<p>
<center>図12.8 。マルチヘッドアテンション層の情報の流れ</center>
</p>
<h3>12.1.7 トランスフォーマー層</h3>
<p>
マルチヘッドセルフアテンションは, トランスフォーマーネットワークの中核となるアーキテクチャ要素を形成します。ニューラルネットワークが深さから大きな恩恵を受けることはわかっているので, 複数のセルフアテンション層を互いに積み重ねたいと考えています。トレーニング効率を向上させるために, マルチヘッド構造をバイパスする残差接続を導入できます。これを行うには, 出力の次元が入力の次元と同じ, つまり \(N×D\) である必要があります。次に, 層の正規化 (Ba , Kiros , およびHinton , 2016) が続き, これによりトレーニングの効率が向上します。結果の変換は次のように記述できます。
\[
Z=LayerNorm[Y(X)+X] \tag{12.20}
\]
ここで, \(Y\) は (12.19) によって定義されます。場合によっては, 層の正規化が, より効果的な最適化をもたらす可能性があるため,
正規化層がマルチヘッドセルフセルフアテンションの後ではなく前に適用されるプレノームに置き換えられます。その場合, 次のようになります。
\[
Z=Y(X^\prime)+X　ここで　X^\prime=LayerNorm[X] \tag{12.21}
\]
いずれの場合も, \(Z\) は入力行列 \(X\) と同じ次元 \(N×D\) です。
</p>
<p>
アテンションメカニズムが値ベクトルの線形結合を作成し, それらが線形結合されて出力ベクトルが生成されることがわかりました。また, 値は入力ベクトルの線形関数であるため, アテンション層の出力は入力の線形結合になるように制約されていることがわかります。非線形性はアテンションウェイトを介して入り込むため, 出力はソフトマックス関数を介して入力に非線形に依存しますが, 出力ベクトルは依然として入力ベクトルが張る部分空間内に存在するように制約されており, これにより, アテンション層の表現力が制限されます。 多層パーセプトロン を意味する MLP[･] と呼ばれる, \(D\) 入力と \(D\) 出力を持つ標準的な非線形ニューラルネットワークを使用して各層の出力を後処理することで, トランスフォーマーの柔軟性を高めることができます。たとえば, これは, ReLU 隠れユニットを備えた 2 層の完全に接続されたネットワークで構成されている場合があります。これは, 可変長のシーケンスを処理するトランスフォーマーの能力を維持する方法で行う必要があります。これを達成するために, Z の行に対応する出力ベクトルのそれぞれに同じ共有ネットワークが適用されます。このニューラルネットワーク層も, 残差接続を使用することで改善できます。また, トランスフォーマー層からの最終出力が次の形式になるように層の正規化も含まれています。
\[
\tilde{X}=LayerNorm[MLP[Z]+Z] \tag{12.22}
\]
これにより, トランスフォーマー層の全体的なアーキテクチャが図12.9 に示され, アルゴリズム12.3 に要約されます。ここでも, 代わりにプレノルムを使用できます。その場合, 最終出力は次のようになります。
\[
\tilde{X}=MLP(Z^\prime)+Z　ここで　Z^\prime=LayerNorm[Z] \tag{12.23}
\]
</p>
<center><img src="images/fig12_9.png"></center>
<p>
図12.9　変換(12.1)を実装するトランスフォーマー アーキテクチャの1つの層 。 ここで, MLP は多層パーセプトロンを表し, "add and norm" は層の正規化が後に続く残差接続を示します。
</p>
<h3>12.1.8 計算複雑度</h3>
<p>
これまで説明したアテンション層は, それぞれ長さ \(D\) の \(N\) 個のベクトルの集合を取得し, それらを同じ次元を持つ別の \(N'\) 個のベクトルの集合にマッピングします。したがって, 入力と出力はそれぞれ全体の次元数 \(ND\) となります。標準の全結合ニューラルネットワークを使用して入力値を出力値にマッピングした場合, \(\mathcal O(N^2D^2)\) 個の独立したパラメーターを持つことになります。同様に, そのようなネットワークを介した1 つの順方向パスを評価する計算コストも \(\mathcal O(N^2D^2)\) になります。
</p>
<p>
アテンション層では, 行列 \(W^{(q)}, W^{(k)}, W^{(v)}\) が入力トークン間で共有されるため, \(D_k\simeq D_v\simeq D\) と仮定すると, 独立パラメータの数は\(\mathcal O(D^2)\) になります。\(N\) 個の入力トークンがあるため, セルフアテンション層の内積を評価する際の計算ステップ数は \(\mathcal O(N^2D)\) です。セルフアテンション層は, 行列の特定のブロック間でパラメータが共有される疎行列と考えることができます。
後続のニューラルネットワーク層には \(D\) 入力と \(D\) 出力があり, コストは \(\mathcal O(D^2)\) です。これはトークン間で共有されるため,  \(N\) で線形な複雑性があり, したがって全体としてこの層のコストは\(\mathcal O(ND^2)\) になります。\(N\)と \(D\) の相対的なサイズに応じて, トランスフォーマー層または MLP層のいずれかが計算コストを支配する可能性があります。全結合されたネットワークと比較して, トランスフォーマー層は計算効率が高くなります。トランスフォーマーアーキテクチャの多くの変形例が提案されています((Lin er al., 2021; Phuong and Hutter, 2022Lin er al., 2021; Phuong and Hutter, 2022))。これには, 効率の向上を目的とした修正も含まれます((Tay et al. 2020Tay et al. 2020))。
</p>
<h3>12.1.9 位置エンコーディング</h3>
<p>
トランスフォーマーアーキテクチャでは, 後続のニューラルネットワークと同様に, 行列Wh q )), Wh k )), およびWh v が入力トークン全体で共有されます。結果として, トランスフォーマには, 入力トークン, つまりX の行の順序を変更すると, 出力行列~X の行も同じ順序で変更されるという特性があります。言い換えれば, トランスフォーマーは入力順列に関して等変です。ネットワークアーキテクチャでのパラメータの共有により, トランスフォーマの大規模な並列処理が容易になり, ネットワークが長距離の依存関係を短距離の依存関係と同じくらい効果的に学習できるようになります。ただし, 自然言語の単語などの連続データを考慮する場合, トークンの順序に依存しないことが大きな制限になります。これは, トランスフォーマーによって学習される表現が入力トークンの順序から独立しているためです。 食べ物は悪く, まったく良くありませんでした。 と 食べ物はおいしかったですが, まったく悪くありませんでした。 という2 つの文 には同じトークンが含まれていますが, トークンの順序が異なるため, 意味が大きく異なります。明らかに, トークンの順序は, 自然言語処理を含むほとんどの逐次処理タスクにとって重要であるため, トークンの順序情報をネットワークに注入する方法を見つける必要があります。
</p>
<p>
私たちは慎重に構築したアテンション層の強力な特性を保持したいため, ネットワークアーキテクチャで表現するのではなく, データ自体にトークンの順序をエンコードすることを目指しています。したがって, 各入力位置nに関連付けられた位置エンコードベクトルrnを構築し, これをxnを埋め込む関連する入力トークンと組み合わせます。これらのベクトルを結合する明白な方法の1つは, それらを連結することですが, これにより入力空間の次元が増加し, したがって後続のすべてのアテンション空間の次元が増加し, 計算コストが大幅に増加します。 代わりに, 単純に位置ベクトルをトークンベクトルに加えて, 次のようにすることができます。
\[
\tag{12.24}
\]
</p>
<p>
これには, 位置エンコードベクトルがトークン埋め込みベクトルと同じ次元を持つことが必要です。
</p>
<p>
最初は, トークンベクトルに位置情報を追加すると入力ベクトルが破損し, ネットワークのタスクがはるかに困難になるように思われるかもしれません。しかし, なぜこれがうまく機能するのかについての直観は, ランダムに選ばれた2つの非相関ベクトルが高次元の空間ではほぼ直交する傾向があることに注目することで得られます。これは, ネットワークがトークンID情報と位置情報を比較的別々に処理できることを示しています。また, 各層にわたる接続が残っているため, あるトランスフォーマー層から次の層に移動する際に位置情報が失われることがないことにも注意してください。さらに, トランスフォーマーの線形処理層により, 連結表現は加算表現と同様の特性を持ちます。
</p>
<p>
次のタスクは, 埋め込みベクトル{rn}を構築することです。簡単なアプローチは, 整数1, 2, 3, ...を各位置に関連付けることです。ただし, これには, 値の大きさが際限なく増加するため, 埋め込みベクトルが大幅に破損し始める可能性があるという問題があります。また, トレーニングで使用されるものより長い新しい入力シーケンスには, トレーニングで使用される値の範囲外にあるコーディング値が含まれるため, うまく汎化できない可能性があります。あるいは, シーケンス内の各トークンに範囲(0,1)の数値を割り当てて, 表現を制限することもできます。ただし, この表現はシーケンス全体の長さに依存するため, 特定の位置に対して一意ではありません。
</p>
<p>
理想的な位置エンコーディングは, 各位置に一意の表現を提供する必要があり, 境界があり, より長いシーケンスに一般化する必要があり, 絶対位置に関係なく2つの入力ベクトル間のステップ数を表現する一貫した方法が必要です。多くの場合, トークンの相対位置は絶対位置よりも重要です。
</p>
<p>
位置エンコーディングには多くのアプローチがあります(Dufter, Schmitt, Schiitze, 2021)。ここでは, Vaswaniら(2017年)によって導入された正弦波関数に基づく手法について説明します。与えられた位置nに対して, 関連する位置エンコーディングベクトルは, 次の式で与えられるコンポーネントrniを持ちます。
\[
\tag{12.25}
\]
</p>
<p>
図12.10(a)に示すように, 埋め込みベクトルrnの要素は, 波長が着実に増加する一連のサイン関数とコサイン関数によって与えられることがわかります。
</p>
<p>
図12.10。(12.25)によって定義され, 位置エンコーディングベクトルの構築に使用される関数の図。(a)横軸が埋め込みベクトルrのさまざまな成分を示し, 縦軸がシーケンス内の位置を示すプロット。2つの位置nおよびmのベクトル要素の値は, 正弦曲線および余弦曲線と灰色の水平線の交点によって示されます。(b)最初のN = 200位置に対する次元D = 100, L = 30に対して(12.25)によって定義された位置エンコーディング ベクトルのヒートマップ図。
</p>
<p>
このエンコーディングには, ベクトルri の要素がすべて(-1, 1)の範囲内にあるという特性があります。これは, 最下位ビットが高い周波数で交互に表示され, 後続のビットが徐々に減少する周波数で交互に表示される, 2進数の表現方法を思い出させます。
</p>
<p>
ただし, (12.25)で与えられるエンコーディングの場合, ベクトル要素はバイナリではなく連続変数です。位置エンコーディングベクトルのプロットを図12.10(b) に示します。
</p>
<p>
(12.25)で与えられる正弦波表現の優れた特性の1つは, 任意の固定オフセットkに対して, 位置n+kでのエンコーディングが位置nでのエンコーディングの線形結合として表現できることです。この場合, 係数は絶対位置ですが, kの値のみに基づいています。したがって, ネットワークは相対的な位置に注意することを学習できる必要があります。このプロパティでは, エンコードでサイン関数とコサイン関数の両方を使用する必要があることに注意してください。
</p>
<p>
位置表現に対するもう1つの一般的なアプローチは, 学習された位置エンコーディングを使用することです。これは, トレーニング中に残りのモデルパラメーターと一緒に学習できる各トークン位置に重みのベクトルを持たせることで実現され, 手作りの表現の使用を回避します。パラメーターはトークンの位置間で共有されないため, 位置エンコーディングの目的である順列の下ではトークンは不変ではなくなります。ただし, トレーニング中に見られない位置エンコーディングについてはエンコーディングがトレーニングされないため, このアプローチは, より長い入力シーケンスに一般化するという前述の基準を満たしていません。したがって, このアプローチは一般に, トレーニング中と推論中の両方で入力長が比較的一定である場合に最も適しています。
</p>
<h2>12.2 自然言語</h2>
<p>
トランスフォーマーのアーキテクチャを学習したので, これを単語, 文, 段落で構成される言語データの処理にどのように使用できるかを検討します。これはトランスフォーマーが元々動作するために開発されたモダリティですが, 非常に一般的なクラスのモデルであることが証明され, ほとんどの入力データタイプにとって最先端のものになりました。この章の後半では, 他のドメインでの使用法について見ていきます。
</p>
<p>
英語を含む多くの言語は, 空白で区切られた一連の単語と句読点記号で構成されているため, 連続データの例を表します。当面は単語に焦点を当てますが, 後で句読点に戻ります。
</p>
<p>
最初の課題は, 単語をディープニューラルネットワークへの入力として使用するのに適した数値表現に変換することです。1 つの簡単なアプローチは, 単語の固定辞書を定義し, 各単語の’ワンホット’表現とともに辞書のサイズに等しい長さのベクトルを導入することです。この場合, 辞書内のk番目の単語は次のようなベクトルでエンコードされます。位置kは1, その他すべての位置は0。たとえば, ‘aardwolf’が辞書の 3 番目の単語である場合, そのベクトル表現は(0, 0, 1 ,0, ...., 0) になります。
</p>
<p>
ワンホット表現に関する明らかな問題は, 現実的な辞書には非常に高次元のベクトルにつながる数十万のエントリが含まれる可能性があることです。また, 単語間に存在する可能性のある類似点や関係性も捕捉しません。どちらの問題も, 各単語が通常数百次元の空間内の密なベクトルとして表現される単語埋め込みと呼ばれるプロセスを通じて, 単語を低次元空間にマッピングすることで解決できます。
</p>
<h3>12.2.1 Word embedding</h3>
<p>
埋め込みプロセスは, サイズD×Kの行列Eによって定義できます。ここで, Dは埋め込み空間の次元数, Kは辞書の次元数です。ワンホットエンコードされた入力ベクトルxnごとに, 次を使用して対応する埋め込みベクトルを計算できます。
\[
\tag{12.26}
\]
</p>
<p>
xnにはワンホットエンコーディングがあるため, ベクトルvnは行列Eの対応する列によって単純に与えられます。
</p>
<p>
テキストのコーパス(つまり, 大規模なデータセット)から行列Eを学習することができ, れを行うためのアプローチは数多くあります。ここでは, 単純な2層のニューラルネットワークとみなすことができる, word2vec(Mikolovら, 2013)と呼ばれる人気のある手法を見ていきます。テキスト内のM個の隣接する単語の’ウィンドウ’を考慮して各サンプルが取得されるトレーニングセットが構築されます。典型的な値はM = 5です。サンプルは独立しているとみなされ, 誤差関数は各サンプルの誤差関数の合計と定義されます。このアプローチには2つのバリエーションがあります。連続した単語のバッグでは, ネットワークトレーニングのターゲット変数は中央の単語であり, 残りのコンテキストワードが入力を形成するため, ネットワークは’空白を埋める’ようにトレーニングされます。スキップグラムと呼ばれる密接に関連したアプローチは, 入力と出力を逆にして, 中心の単語が入力として表示され, ターゲット値がコンテキスト単語になるようにします。これらのモデルを図12.11に示します。
</p>
<p>
図12.11。単語埋め込みの学習に使用される2層ニューラルネットワーク。(a)は連続バッグオブワードアプローチを示し, (b)はスキップグラムアプローチを示します。
</p>
<p>
このトレーニング手順は, 自己教師あり学習の一種とみなすことができます。データは, 単語シーケンスの多数の小さなウィンドウがランダムに抽出される, ラベルのないテキストの大きなコーパスで構成されているだけだからです。ラベルは, ネットワークが値を予測しようとしている単語を’マスク’することによって, テキスト自体から取得されます。
</p>
<p>
モデルがトレーニングされると, 埋め込み行列Eは, 連続バッグオブワード手法の場合は第2層の重み行列の転置によって, スキップグラムの場合は第1層の重み行列によって与えられます。意味的に関連する単語は埋め込み空間内の近くの位置にマッピングされます。関連する単語は, 無関係な単語と比較して, 同様の文脈の単語で出現する可能性が高いため, これは予想されることです。たとえば, ‘都市’や’首都’という単語は, ‘パリ’や’ロンドン’などのターゲット単語のコンテキストとしてより高い頻度で出現する可能性がありますが, ‘オレンジ’や’多項式’のコンテキストとしてはあまり出現しない可能性があります。’パリ’と’ロンドン’が近くの埋め込みベクトルにマッピングされている場合, ネットワークは欠落単語の確率をより簡単に予測できます。
\[
\tag{12.27}
\]
</p>
<p>
Word Embeddingは, もともとそれ自体が自然言語処理ツールとして開発されました。現在では, ディープニューラルネットワークの前処理ステップとして使用される可能性が高くなります。この点において, それらはディープニューラルネットワークの最初の層とみなすことができます。これらは, 標準的な事前トレーニング済み埋め込み行列を使用して修正することも, システムの全体的なエンドツーエンドトレーニングの一部として学習される適応層として扱うこともできます。後者の場合, 埋め込み層は, ランダムな重み値または標準の埋め込み行列を使用して初期化できます。
</p>
<h3>12.2.2 Tokenization</h3>
<p>
固定の単語辞書を使用する場合の問題の1つは, 辞書にない単語やスペルが間違っている単語に対処できないことです。また, 句読点記号やコンピューターコードなどの他の文字シーケンスも考慮されません。これらの問題に対処する別のアプローチは, 単語を使用する代わりに文字レベルで作業することです。これにより, 辞書は大文字と小文字, 数字, 句読点, およびスペースやタブなどの空白記号で構成されます。ただし, このアプローチの欠点は, 言語の意味的に重要な単語構造が破棄され, 後続のニューラルネットワークが基本文字から単語を再構成することを学習する必要があることです。また, 特定のテキスト本文に対してはるかに多くの連続ステップが必要になるため, シーケンスを処理するための計算コストが増加します。
</p>
<p>
単語と句読点記号の文字列をトークンの文字列に変換する前処理ステップを使用することで, 文字レベルと単語レベルの表現の利点を組み合わせることができます。トークンは通常, 小さな文字グループであり, その中に一般的な単語が含まれる場合があります。全体に加えて, より長い単語の断片や, あまり一般的ではない単語に組み立てることができる個々の文字も含まれます(Schusterと中島, 2012)。このトークン化により, システムはコンピューターコードや画像などの他のモダリティなどの他の種類のシーケンスも処理できるようになります。これは, 同じ単語のバリエーションが関連する表現を持つ可能性があることも意味します。たとえば, ‘cook’, ‘cooks’, ‘cooked’, ‘cooking’, および’cooker’はすべて関連しており, 共通要素’cook’を共有しています。これ自体はトークンの1つとして表すことができます。
</p>
<p>
トークン化にはさまざまなアプローチがあります。一例として, データ圧縮に使用されるバイトペアエンコーディングと呼ばれる技術は, バイトの代わりに文字を結合することでテキストのトークン化に適用できます(Sennrich, Haddow, および Birch, 2015)。このプロセスは個々の文字から始まり, それらを繰り返しマージしてより長い文字列を作成します。トークンのリストは, 最初に個々の文字のリストで初期化されます。次に, テキストの本文で最も頻繁に発生する隣接するトークンのペアが検索され, これらのペアが新しいトークンに置き換えられます。単語がマージされないようにするため, 2番目のトークンが空白で始まる場合, 2つのトークンから新しいトークンは形成されません。図12.12に示すように, このプロセスは繰り返し繰り返されます。
</p>
<p>
図12.12。バイトペアエンコーディングと同様に自然言語をトークン化するプロセスの図。この例では, 最も頻繁に出現する文字のペアは’pe’で, 4回出現するため, これらは出現するすべての’pe’を置き換える新しいトークンを形成します。大文字の’P’と小文字の’p’は別の文字です。次に, ‘ck’のペアが3回発生するため追加されます。これに’pi’, ‘ed’, ‘per’などのトークンが続き, これらはすべて2回出現します。
</p>
<p>
初期状態では, トークンの数は文字の数と等しく, 比較的小さいです。トークンが形成されると, トークンの総数が増加し, これが十分に長く続くと, 最終的にトークンはテキスト内の単語のセットに対応します。トークンの総数は通常, 文字レベルと単語レベルの表現の間の妥協として, 事前に固定されます。このトークン数に達すると, アルゴリズムは停止します。
</p>
<p>
自然言語への深層学習の実際の応用では, 通常, 入力テキストは最初にトークン化された表現にマッピングされます。ただし, この章の残りの部分では, 重要な概念を説明し動機づけるのが容易になるため, 単語レベルの表現を使用します。
</p>
<h3>12.2.3. Bag of words</h3>
<p>
次に, 自然言語の単語(またはトークン)などのベクトルの順序付けされたシーケンスの結合分布p(x1, ..., xN)をモデル化するタスクに移ります。最も単純なアプローチは, 単語が同じ分布から独立して描画され, したがって結合分布が次の形式で完全に因数分解されると仮定することです。
\[
\tag{12.28}
\]
</p>
<p>
これは, ノードが相互接続するリンクを持たずに分離されている確率的グラフィカルモデルとして表現できます。
</p>
<p>
分布p(x)は変数間で共有され, 一般性を失うことなく, xの可能な各状態の確率をリストした単純なテーブル(単語またはトークンの辞書に対応)として表すことができます。このモデルの最尤解は, これらの確率のそれぞれをトレーニングセット内で単語が出現する回数の割合に設定するだけで得られます。これは, 単語の順序を完全に無視するため, バグオブワードモデルとして知られています。
</p>
<p>
バッグオブワードアプローチを使用して, 単純なテキスト分類子を構築できます。これは, たとえば, レストランのレビューを表すテキストの一節を肯定的または否定的に分類するセンチメント分析に使用できます。単純ベイズ分類器は, 単語が各クラスCk内で独立しているが, クラスごとに異なる分布を持つと仮定します。
\[
\tag{12.29}
\]
</p>
<p>
事前クラス確率p(Cx)が与えられると, 新しいシーケンスの事後クラス確率は次のように求められます。
\[
\tag{12.30}
\]
</p>
<p>
クラス条件付き密度p(x|Ck)と事前確率p(Ck)は両方とも, トレーニングデータセットからの頻度を使用して推定できます。新しいシーケンスの場合, テーブルのエントリが乗算されて, 必要な事後確率が得られます。トレーニングセットに存在しなかった単語がテストセット内に出現した場合, 対応する確率推定値はゼロになるため, これらの推定値は通常, トレーニング後にすべてのエントリに均一に小さなレベルの確率を再割り当てしてゼロ値を回避することによって’平滑化’されることに注意してください。
</p>
<h3>12.2.4 Autoregressive models</h3>
<p>
Bag-of-Words モデルの明らかな大きな制限の1つは, 語順を完全に無視することです。これに対処するために, 自己回帰アプローチを採用できます。一般性を失うことなく, 単語のシーケンスにわたる分布を次の形式の条件付き分布の積に分解できます。
\[
\tag{12.31}
\]
</p>
<p>
これは, シーケンス内の各ノードが前のすべてのノードからリンクを受け取る確率的グラフィカルモデルとして表すことができます。(12.31)の右側の各項をテーブルで表すことができます。テーブルのエントリは, トレーニングセットからの単純な頻度カウントを使用して再度推定されます。ただし, これらのテーブルのサイズはシーケンスの長さに応じて指数関数的に増加するため, このアプローチは法外に高価になります。
</p>
<p>
(12.31) の右側の条件付き分布のそれぞれが, 最近のL語を除くすべての以前の観測から独立していると仮定することで, モデルを大幅に単純化できます。たとえば, L = 2の場合, このモデルに基づくN個の観測値のシーケンスの同時分布は次のようになります。
\[
\tag{12.32}
\]
</p>
<p>
対応するグラフィカルモデルでは, 各ノードには前の2つのノードからのリンクがあります。ここでは, 条件付き分布p(xn |xn-1) がすべての変数で共有されると仮定します。ここでも, (12.32)の右側の各分布は, トレーニングコーパスから抽出された連続する単語の3つ組の統計から値が推定されるテーブルとして表すことができます。
</p>
<p>
L = 1の場合は, 隣接する単語のペアに依存するため, バイグラムモデルとして知られています。同様に, 隣接する単語の3つ組を含むL = 2をトライグラムモデルと呼び, 一般にこれらをnグラムモデルと呼びます。
</p>
<p>
これらの推定値は, 通常, ゼロ値を避けるために, すべてのエントリにわたって均一に小さなレベルの確率を再割り当てすることによって, トレーニング後に’平滑化’されます。このセクションでこれまで説明したすべてのモデルは, 生成的に実行して新しいテキストを合成できます。たとえば, シーケンス内の最初と2番目の単語を指定すると, トライグラム統計p(xn|xn-1, xn-2)からサンプリングして3番目の単語を生成し, その後2番目の単語を使用できます。3番目の単語を使用して4番目の単語をサンプリングする, というようになります。ただし, 各単語は前の2つの単語に基づいてのみ予測されるため, 結果のテキストは支離滅裂になります。高品質のテキストモデルでは, 言語の長期的な依存関係を考慮する必要があります。一方, 確率テーブルのサイズはLにおいて指数関数的に増大するため, Lの値を単純に増やすことはできません。そのため, トライグラムモデルをはるかに超えると法外なコストがかかります。ただし, 確率テーブルではなく, トランスフォーマーとして構成されたディープニューラルネットワークに基づいて現代言語モデルを検討する場合, 自己回帰表現が中心的な役割を果たします。
</p>
<p>
n-gramモデルのパラメーター数の指数関数的な増加を避けながら, より広範囲の依存関係を許可する1つの方法は, 図11.31に示すグラフィック構造を持つ隠れマルコフモデルを使用することです。学習可能なパラメータの数は潜在変数の次元によって支配されますが, 特定の観測値xnにわたる分布は, 原則として, 以前のすべての観測値に依存します。しかし, より遠い観測の影響は, それ自体がより最近の観測によって更新される潜在状態の連鎖を通じて伝達される必要があるため, 依然として非常に限定されています。
</p>
<h3>12.2.5 Recurrent neural networks</h3>
<p>
n-gramなどの手法は, 条件付き分布の完全に一般的なテーブルを格納するため, シーケンス長のスケーリングが非常に不十分です。ニューラルネットワークに基づいたパラメーター化されたモデルを使用することで, より優れたスケーリングを実現できます。標準的なフィードフォワードニューラルネットワークを自然言語の単語のシーケンスに単純に適用すると仮定します。 発生する1つの問題は, ネットワークの入力と出力の数が固定であるのに対し, トレーニングセットとテストセット内の可変長のシーケンスを処理できる必要があることです。さらに, シーケンス内の特定の位置にある単語または単語のグループが何らかの概念を表す場合, 別の位置にある同じ単語または単語のグループは, その新しい位置でも同じ概念を表す可能性があります。これは, 画像データの処理中に遭遇した等分散特性を思い出させます。シーケンス全体でパラメーターを共有できるネットワークアーキテクチャを構築できれば, この等分散特性を捉えることができるだけでなく, モデル内の自由パラメーターの数を大幅に削減でき, 異なる長さのシーケンスを処理することもできます。
</p>
<p>
これに対処するために, 隠れマルコフ モデルからインスピレーションを借りて, シーケンス内の各ステップnに関連付けられた明示的な隠れ変数znを導入できます。 ニューラルネットワークは, 現在の単語xnと現在の隠れ状態zn-1の両方を入力として受け取り, 出力単語ynと隠れ変数の次の状態Znを生成します。次に, このネットワークのコピーをチェーンして, 重み値をコピー間で共有することができます。結果として得られるアーキテクチャはリカレントニューラルネットワーク(RNN) と呼ばれ, 図12.13に示されています。ここで, 隠れ状態の初期値は, たとえば, z0 = (0, 0, ..., 0)Tなどのデフォルト値に初期化できます。
</p>
<p>
図12.13。パラメータwを持つ一般的なRNN。シーケンスx1, ..., xNを入力として受け取り, シーケンスy1, ..., yNを出力として生成します。各ボックスは, 非線形隠れユニットを備えた多層ネットワークに対応します。
</p>
<p>
RNNが実際にどのように使用されるかを示す例として, 文章を英語からオランダ語に翻訳するという具体的なタスクを考えてみましょう。文は可変長にすることができ, 各出力文は対応する入力文とは異なる長さになる場合があります。さらに, ネットワークは, 出力文の生成を開始する前に, 入力文全体を確認する必要がある場合があります。RNNを使用して, 完全な英語の文を入力し, その後に(開始)で示す特別な入力トークンを入力して, 翻訳の開始をトリガーすることで, この問題に対処できます。トレーニング中に, ネットワークは出力文の先頭との関連付け(開始)を学習します。また, 図12.14に示すように, 連続して生成された各単語を次のタイムステップの入力に入力します。ネットワークは, 翻訳の完了を示す特定の(停止)トークンを生成するようにトレーニングできます。ネットワークの最初の数ステージは入力シーケンスを吸収するために使用され, 関連する出力ベクトルは単純に無視されます。ネットワークのこの部分は, 入力文全体が隠れ変数の状態z*に圧縮されている’エンコーダー’とみなすことができます。残りのネットワークステージは’デコーダ’として機能し, 翻訳された文を一度に1単語ずつ出力として生成します。各出力ワードがネットワークの次のステージへの入力として供給されるため, このアプローチは(12.31)に類似した自己回帰構造を持つことに注意してください。
</p>
<p>
図12.14。言語翻訳に使用されるリカレントニューラルネットワークの例。詳細については本文を参照してください。
</p>
<h3>12.2.6 Backpropagation through time</h3>
<p>
RNNは, 通常のニューラルネットワークと同様に, 逆伝播によって計算された勾配を使用した確率的勾配降下によってトレーニングし, 自動微分によって評価できます。誤差関数は, 各ユニットの誤差のすべての出力ユニットにわたる合計で構成されます。各出力ユニットには, 関連するクロスエントロピー誤差関数とともにsofimax活性化関数があります。順方向伝播中, アクティベーション値はシーケンスの最初の入力からシーケンスのすべての出力ノードまで伝播され, その後エラー信号が同じパスに沿って逆伝播されます。このプロセスは時間によるバックプロパゲーションと呼ばれ, 原理的には簡単です。ただし, 実際には, 非常に長いシーケンスの場合, 非常に深いネットワークアーキテクチャで発生する勾配の消失や勾配の爆発の問題により, トレーニングが困難になる可能性があります。
</p>
<p>
標準RNNのもう1つの問題は, 長距離の依存関係の処理が不十分であることです。これは, そのような依存関係が広範囲に及ぶ自然言語では特に問題になります。テキストの長い文章では, テキストのかなり後の方に出現する単語を予測する上で重要な役割を果たす概念が導入される場合があります。図12.14に示すアーキテクチャでは, 英語の文の概念全体を固定長の単一の隠れベクトルz*に取り込む必要がありますが, これはシーケンスが長くなるとますます問題になります。これはボトルネック問題として知られています。これは, 任意の長さのシーケンスをアクティベーションの単一の隠れベクトルに要約する必要があり, ネットワークは完全な入力シーケンスが処理された後でのみ出力変換の生成を開始できるためです。
</p>
<p>
勾配の消失と爆発の問題, および限定的な長距離依存関係の両方に対処する1つのアプローチは, ニューラルネットワークのアーキテクチャを変更して, ネットワークの各ステージ内の多くの処理ステップをバイパスする追加の信号パスを許可し, 情報を送信できるようにすることです。より多くのタイムステップにわたって記憶されます。長短期記憶(LSTM)モデル (Hochreiter および Schmidhuber, 1997)とゲート反復単位(GRU)モデル(Cho et al., 2014)が最も広く知られている例です。標準 RNNと比較してパフォーマンスは向上しますが, 長距離の依存関係をモデル化する能力は依然として限られています。 また, 各セルの複雑さが増すため, LSTMのトレーニングは標準のRNNよりもさらに遅くなります。さらに, すべてのリカレントネットワークには, シーケンス内のステップ数に応じて直線的に増加する信号パスがあります。さらに, 処理の逐次的な性質により, 単一のトレーニング サンプル内での並列計算はサポートされません。これは特に, RNNがGPUをベースとした最新の高度な並列ハードウェアを効率的に使用するのに苦労していることを意味します。これらの問題は, RNNをトランスフォーマーに置き換えることによって解決されます。
</p>
<h2>12.3 Transformer Language Models</h2>
<p>
トランスフォーマー処理層は, 幅広い適用性を持つ強力なニューラルネットワークモデルを構築するための非常に柔軟なコンポーネントです。このセクションでは, 自然言語へのトランスフォーマーの応用を検討します。これにより, 大規模言語モデル(LLM)として知られる大規模なニューラルネットワークの開発が生まれ, 非常に有能であることが証明されました(Zhao et al., 2023)。
</p>
<p>
トランスフォーマーはさまざまな種類の言語処理タスクに適用でき, 入出力データの形式に応じて3つのカテゴリに分類できます。センチメント分析などの問題では, 一連の単語を入力として受け取り, テキストのセンチメント(幸せまたは悲しいなど) を表す単一の変数を出力として提供します。ここでは, トランスフォーマーがシーケンスの’エンコーダー’として機能します。他の問題では, たとえば, 入力画像からテキストキャプションを生成したい場合など, 単一のベクトルを入力として受け取り, 単語シーケンスを出力として生成する場合があります。このような場合, トランスフォーマーは’デコーダー’として機能し, 出力としてシーケンスを生成します。 最後に, シーケンスツーシーケンス処理タスクでは, たとえば, ある言語から別の言語に翻訳することが目的の場合, 入力と出力の両方が一連の単語で構成されます。この場合, トランスフォーマーはエンコーダーとデコーダーの両方の役割で使用されます。モデル アーキテクチャの具体的な例を使用して, 言語モデルのこれらの各クラスについて順番に説明します。
</p>
<h3>12.3.1 Decoder transformers</h3>
<p>
まず, デコーダ専用のトランスフォーマーモデルを検討します。これらは, トークンの出力シーケンスを作成する生成モデルとして使用できます。説明的な例として, Generative pre-trained Transformerを表すGPTと呼ばれるモデルのクラスに焦点を当てます(Radford et al., 2019; Brown et al., 2020; OpenAl, 2023)。目標は, トランスフォーマーアーキテクチャを使用して, 条件付き分布p(xn| x1, ..., xn-1)が学習されたトランスフォーマーニューラルネットワークを使用して表現される(12.31)で定義される形式の自己回帰モデルをデータから構築することです。 
</p>
<p>
このモデルは, 最初のn-1個のトークンで構成されるシーケンスを入力として受け取り, その対応する出力はトークンnの条件付き分布を表します。この分布からサンプルを抽出すると, シーケンスがn個のトークンに拡張され, この新しいシーケンスをモデルを通じてフィードバックして, トークンn+1にわたる分布を与えることができます。このプロセスを繰り返して, トランスフォーマーへの入力数によって決定される最大長までシーケンスを生成できます。条件付き分布からのサンプリング戦略についてはすぐに説明しますが, 現時点ではネットワークを構築してトレーニングする方法に焦点を当てます。
</p>
<p>
GPTモデルのアーキテクチャは, 次元Dのトークンのシーケンスx1, ..., xNを入力として受け取り, トークンのシーケンス ~x1, ..., ~xNを生成するトランスフォーマー層のスタックで構成されます。再び次元Dを出力として返します。各出力は, そのタイムステップでのトークンの辞書にわたる確率分布を表す必要があり, この辞書の次元はKであるのに対し, トークンの次元はDです。したがって, 行列 W' を使用して各出力トークンの線形変換を行います。 次元D×Kの後に次の形式のソフトマックス活性化関数が続きます。
\[
\tag{12.23}
\]
</p>
<p>
ここで, Yはn行目がynTである行列, ~Xはn行目が~xnTである行列です。各ソフトマックス出力ユニットには, 関連するクロスエントロピー誤差関数があります。モデルのアーキテクチャを図12.15に示します。
</p>
<p>
図12.15。GPTデコーダ変換ネットワークのアーキテクチャ。ここで, ‘LSM’は線形ソフトマックスの略で, 学習可能なパラメータがトークン位置全体で共有され, その後にソフトマックス活性化関数が続く線形変換を表します。マスキングについては本文中で説明しています。
</p>
<p>
モデルは, 自己教師ありアプローチを採用することで, ラベルのない自然言語の大規模なコーパスを使用してトレーニングできます。各トレーニングサンプルは, ネットワークへの入力を形成するトークンx1, ..., xNのシーケンスと, シーケンス内の次のトークンで構成される関連するターゲット値xn+1で構成されます。シーケンスは独立しており, 同一に分布しているとみなされるため, トレーニングに使用される誤差関数は, トレーニングセット全体で合計され, 適切なミニバッチにグループ化されたクロスエントロピー誤差値の合計になります。単純に, モデルを通過する前方パスを使用して, このような各トレーニングサンプルを個別に処理できます。ただし, シーケンス全体を一度に処理して, 各トークンが前のトークンのシーケンスのターゲット値と後続のトークンの入力値の両方として機能するようにすることで, はるかに高い効率を実現できます。たとえば, 次の単語列を考えてみましょう。
\[
I　swam　across　the　river　to　get　to　the　other　bank.
\]
</p>
<p>
関連するターゲット’the’を持つ入力シーケンスとして’I swam across the’を使用できます。また, ‘river’という関連するターゲットを持つ入力シーケンスとして’I swam across the’を使用することもできます。ただし, これらを並行して処理するには, ネットワークがシーケンスを先読みして’不正’できないようにする必要があります。そうしないと, 次の入力を出力に直接コピーすることになります。これを行うと, 定義上, 後続のトークンがテスト時に利用できないため, 新しいシーケンスを生成できなくなります。この問題に対処するために, 私たちは2つのことを行います。まず, 入力シーケンスを1ステップ右にシフトし, ターゲットxn+1で入力xnが出力yn+1に対応するようにします。また, (start)と示される追加の特別なトークンが入力シーケンスの最初の位置に付加されます。次に, トランスフォーマー内のトークンは, アテンションの重みを計算するために使用される場合を除き, ドット積を通じてペアで相互作用する場合を除き, 独立して処理されることに注意してください。したがって, 我々は, 場合によっては因果的注意と呼ばれる, マスクされた注意を各注意層に導入し, シーケンス内の後のトークンに付随するトークンに対応するすべての注意の重みをゼロに設定します。これには, (12.14)で定義されたアテンション行列Attendant(Q, K, V)の対応する要素をすべてゼロに設定し, 各行の合計が再び1になるように残りの要素を正規化するだけです。実際には, これは, 対応するアクティブ化前の値を-∞に設定することで実現できます。これにより, 関連する出力のソフトマックスが0と評価され, 非ゼロ出力全体の正規化も処理されます。マスクされたアテンションマトリックスの構造を図12.16に示します。
</p>
<p>
図12.16。マスクされた自己注意のマスク行列の図。赤色の要素に対応する注意の重みは0に設定されます。したがって, トークン’across’を予測する場合, 出力は入力トークン’(start)’’I’および’swam’にのみ依存できます。
</p>
<p>
実際には, GPUの大規模な並列処理を効率的に利用したいため, 複数のシーケンスを1つの入力テンソルにスタックして, 単一のバッチで並列処理することができます。ただし, テキストシーケンスは当然可変長であるのに対し, これにはシーケンスが同じ長さである必要があります。これは, (パッド)で示される特定のトークンを導入することで対処できます。これは, 未使用の位置を埋めてすべてのシーケンスを同じ長さにして単一のテンソルに結合できるようにするために使用されます。次に, 出力ベクトルが(パッド)トークンによって占有されている入力に注意を払わないようにするために, 注意の重みで追加のマスクが使用されます。このマスクの形式は特定の入力シーケンスに依存することに注意してください。
</p>
<p>
トレーニングされたモデルの出力は, ソフトマックス出力アクティベーション関数によって与えられるトークン空間にわたる確率分布であり, 現在のトークンシーケンスが与えられた場合の次のトークンの確率を表します。この次の単語が選択されると, 新しいトークンが含まれたトークンシーケンスをモデルに再度供給して, シーケンス内の後続のトークンを生成できます。このプロセスは無期限に, またはシーケンス終了トークンが生成されるまで繰り返すことができます。これは, 新しく生成されたトークンごとにモデル全体にデータを供給する必要があるため, 非常に非効率的に見えるかもしれません。ただし, マスクされたアテンションにより, 特定のトークンに対して学習された埋め込みは, そのトークン自体と以前のトークンにのみ依存するため, 新しい後のトークンが生成されても変更されないことに注意してください。その結果, 新しいトークンを処理するときに, 計算の大部分をリサイクルできます。
</p>
<h3>12.3.2 Sampling strategies</h3>
<p>
デコーダトランスフォーマの出力はシーケンス内の次のトークンの値の確率分布であり, シーケンスを拡張するにはそこからそのトークンの特定の値を選択する必要があることがわかりました。計算された確率に基づいてトークンの値を選択するには, いくつかのオプションがあります(Holtzman et al., 2019)。貪欲検索と呼ばれる明白なアプローチの1つは, 最も高い確率でトークンを選択することです。これには, 特定の入力シーケンスが常に同じ出力シーケンスを生成するという点で, モデルが決定論的になる効果があります。各ステージで単に最高確率のトークンを選択することは, 最高確率のトークンのシーケンスを選択することと同じではないことに注意してください。最も可能性の高いシーケンスを見つけるには, すべてのトークンにわたる結合分布を最大化する必要があります。これは次のように与えられます。
\[
\tag{12.34}
\]
</p>
<p>
シーケンス内にN個のステップがあり, 辞書内のトークン値の数がKの場合, シーケンスの総数はO(KN) であり, シーケンスの長さに応じて指数関数的に増加するため, 最も可能性の高い単一のシーケンスを見つけることは実現不可能になります。 比較すると, 貪欲探索のコストはO(KN)であり, シーケンスの長さは線形です。
</p>
<p>
貪欲探索よりも高い確率のシーケンスを生成する可能性のある手法の1つは, ビーム探索と呼ばれます。各ステップで最も可能性の高い単一のトークン値を選択する代わりに, 一連のB仮説を維持します。Bはビーム幅と呼ばれ, 各仮説はステップnまでの一連のトークン値で構成されます。次に, これらすべてのシーケンスをネットワーク経由でフィードし, シーケンスごとにB個の最も可能性の高いトークン値を見つけます。これにより, 拡張されたシーケンスに対してB2の可能な仮説が作成されます。次に, このリストは, 拡張されたシーケンスの合計確率に従って, 最も可能性の高いB仮説を選択することによって枝刈りされます。したがって, ビーム探索アルゴリズムはB個の代替シーケンスを維持し, それらの確率を追跡し, 最終的に考慮されたシーケンスの中から最も可能性の高いシーケンスを選択します。シーケンスの確率はシーケンスの各ステップでの確率を乗算することによって取得され, これらの確率は常に1以下であるため, 一般に長いシーケンスは短いシーケンスよりも確率が低くなり, 結果が短いシーケンスに偏ります。このため, シーケンスの確率は通常, 比較を行う前に対応するシーケンスの長さによって正規化されます。ビーム探索のコストはO(BKN)で, これもシーケンス長が線形です。ただし, シーケンスを生成するコストはB倍に増加するため, 非常に大規模な言語モデルの場合, 推論のコストが大幅に増加する可能性があるため, ビーム探索の魅力が大幅に低下します。
</p>
<p>
貪欲サーチやビームサーチなどのアプローチの問題の1つは, 潜在的な出力の多様性が制限され, 生成プロセスがループに陥り, 同じ単語のサブシーケンスが何度も繰り返される可能性があることです。図 12.17 からわかるように, 人間が生成したテキストは確率が低いため, 特定のモデルに関しては自動生成されたテキストよりも驚くべきものになる可能性があります。
</p>
<p>
図12.17。特定のトレーニング済みトランスフォーマー言語モデルと特定の初期入力シーケンスに対するビーム検索と人間のテキストからのトークン確率の比較。人間のシーケンスのトークン確率がいかに低いかを示しています。 [ホルツマンらより (2019)]
</p>
<p>
最も高い確率でシーケンスを見つけようとする代わりに, 各ステップでソフトマックス分布からサンプリングするだけで連続したトークンを生成できます。ただし, これでは無意味なシーケンスが発生する可能性があります。これは, 通常, トークン ディクショナリのサイズが非常に大きいことから発生します。トークンディクショナリには, それぞれの確率が非常に小さいものの, 合計すると確率全体のかなりの部分を占める多くのトークン状態のロング テールが存在します。これは, システムが次のトークンに対して間違った選択をする可能性が非常に高いという問題を引き起こします。
</p>
<p>
これらの両極端の間のバランスとして, Kの選択に対して上位K個の確率を持つ状態のみを考慮し, 繰り込み確率に従ってこれらからサンプリングすることができます。このアプローチの変形は, top-pサンプリングまたはニュークリアスサンプリングと呼ばれ, しきい値に達するまで上位出力の累積確率を計算し, その後, この制限されたトークン状態のセットからサンプリングします。
</p>
<p>
トップKサンプリングの’ソフト’バージョンは, 温度と呼ばれるパラメータ T をソフトマックス関数の定義に導入することです (Hinton, Vinyals, および Dean, 2015)。
\[
\tag{12.35}
\]
</p>
<p>
そして, この変更された分布から次のトークンをサンプリングします。T = 0の場合, 確率の塊は最も確率の高い状態に集中し, 他のすべての状態の確率はゼロになるため, これは貪欲な選択になります。T = 1の場合, 未修正のソフトマックス分布が回復され, T→∞になると, 分布はすべての状態にわたって均一になります。0 < T < 1の範囲内の値を選択すると, 確率はより高い値に集中します。
</p>
<p>
シーケンス生成に関する1つの課題は, 学習フェーズでは人間が生成した入力シーケンスでモデルがトレーニングされるのに対し, 生成的に実行されている場合は入力シーケンス自体がモデルから生成されることです。これは, モデルがトレーニング中に見られるシーケンスの分布から離れる可能性があることを意味します。
</p>
<h3>12.3.3 Encoder transformers</h3>
<p>
次に, エンコーダーに基づくトランスフォーマー言語モデルについて検討します。エンコーダーは, シーケンスを入力として受け取り, クラスラベルなどの固定長ベクトルを出力として生成するモデルです。このようなモデルの例としては, トランスフォーマーからの双方向エンコーダー表現を表すBERTがあります(Devlinら 2018)。目標は, 大規模なテキストコーパスを使用して言語モデルを事前トレーニングし, 次に, より小規模なアプリケーション固有のトレーニング データセットを必要とする幅広い下流タスクに対して転移学習を使用してモデルを微調整することです。エンコーダトランスのアーキテクチャを図 12.18に示します。このアプローチは, 前に説明したトランス層を直接適用したものです。
</p>
<p>
すべての入力文字列の最初のトークンは特別なトークン(クラス)によって与えられ, 対応するモデルの出力は事前トレーニング中に無視されます。その役割は, 微調整について説明するときに明らかになります。モデルは, 入力でトークンシーケンスを提示することによって事前トレーニングされます。ランダムに選択されたトークンのサブセット(たとえば15%)は, (マスク) で示される特別なトークンに置き換えられます。モデルは, 対応する出力ノードで欠落しているトークンを予測するようにトレーニングされます。これは, word2vecで単語の埋め込みを学習するために使用されるマスキングに似ています。たとえば, 入力シーケンスは次のようになります。
\[
I　(mask)　across　the　river　to　get　to　the　(mask)　bank.
\]
そして, ネットワークは出力ノード 2 で’swam’を予測し, 出力ノード10で’other’を予測する必要があります。この場合, 出力のうち 2 つだけが誤差関数に寄与し, 他の出力は無視されます。
</p>
<p>
図12.18。エンコーダトランスモデルのアーキテクチャ。’LSM’とラベル付けされたボックスは, 学習可能なパラメータがトークン位置全体で共有され, その後にソフトマックス活性化関数が続く線形変換を示します。デコーダーモデルと比較した主な違いは, 入力シーケンスが右にシフトされず, ‘先読み’マスキングマトリックスが省略されるため, 各セルフアテンションレイヤー内で, すべての出力トークンが任意の入力トークンに対応できることです。
</p>
<p>
‘双方向’という用語は, ネットワークがマスクされた単語の前後の両方の単語を認識し, 両方の情報ソースを使用して予測を行うことができるという事実を指します。その結果, デコーダーモデルとは異なり, 入力を1桁右にシフトする必要はなく, シーケンスの後半で発生する入力トークンが表示されないように各層の出力をマスクする必要もありません。デコーダーモデルと比較すると, エンコーダーはシーケンス トークンの一部のみがトレーニングラベルとして使用されるため, 効率が低くなります。さらに, エンコーダ モデルはシーケンスを生成できません。
</p>
<p>
ランダムに選択されたトークンを(マスク)に置き換える手順は, 後続の微調整セットに(マスク)トークンが含まれないという点で, トレーニングセットに不一致があることを意味します。これによって引き起こされる可能性のある問題を軽減するために, Devlin et al. (2018)は手順をわずかに変更し, ランダムに選択されたトークンの15%のうち80%が(マスク)に置き換えられ, 10% が語彙からランダムに選択された単語に置き換えられ, 10%のケースでは,  元の単語は入力時に保持されますが, 出力時に正しく予測される必要があります。
</p>
<p>
エンコーダーモデルがトレーニングされると, さまざまなタスクに合わせて微調整できます。これを行うために, 解決されるタスクに固有の形式を持つ新しい出力層が構築されます。テキスト分類タスクの場合, 最初の出力位置のみが使用されます。これは, 入力シーケンスの最初の位置に常に現れる(クラス)トークンに対応します。この出力の次元がDの場合, 次元D×K(K はクラスの数)のパラメーターの行列が最初の出力ノードに追加され, これが K次元のsofimax関数または次元のベクトルに入力されます。D×1の後に, K = 2のロジスティックシグモイドが続きます。線形出力変換は, MLPなどのより複雑な微分可能なモデルに置き換えることもできます。目的が入力文字列の各トークンを分類することである場合, たとえば各トークンをカテゴリ(人物, 場所, 色など)に割り当てることである場合, 最初の出力は無視され, 後続の出力には共有のリニアプラスソフトマックス層が含まれます。微調整中に, 新しい出力行列を含むすべてのモデルパラメーターが, 正しいラベルの対数確率を使用した確率的勾配降下法によって学習されます。あるいは, 事前トレーニングされたモデルの出力が, テキストから画像への合成などのアプリケーション向けの高度な生成深層学習モデルに入力される可能性があります。
</p>
<h3>12.3.4 Sequence-to-sequence transformers</h3>
<p>
完全を期すために, Vaswaniら(2017年)のオリジナルのトランス論文で説明されているように, エンコーダとデコーダを組み合わせたトランスフォーマーモデルの3番目のカテゴリについて簡単に説明します。英語の文をオランダ語の文に翻訳するタスクを考えてみましょう。前述したように, デコーダーモデルを使用して, オランダ語の出力に対応するトークンシーケンスをトークンごとに生成できます。主な違いは, この出力が英語の文に対応する入力シーケンス全体に基づいて条件付けされる必要があることです。エンコーダトランスフォーマを使用して, 入力トークンシーケンスを適切な内部表現にマッピングできます。これをZで示します。Zを出力シーケンスの生成プロセスに組み込むために, クロスアテンションと呼ばれる修正形式のアテンションメカニズムを使用します。これはセルフアテンションと同じですが, 図12.19に示すように, クエリ ベクトルは生成されるシーケンス(この場合はオランダ語出力シーケンス)から取得されますが, キーベクトルと値ベクトルはZで表されるシーケンスから取得されます。ビデオストリーミングサービスのアナロジーに戻ると, これは, ユーザーがクエリベクトルを別のストリーミング会社に送信し, そのクエリベクトルを独自のキーベクトルのセットと比較して最適な一致を見つけるようなものです。そして, 関連付けられた値ベクトルをムービーの形式で返します。
</p>
<p>
図12.19。シーケンストゥーシーケンス変換器のデコーダセクションで使用される1つのクロスアテンション層の概略図。ここで, Zはエンコーダ部からの出力を示します。 Zはクロスアテンション層のキーベクトルと値ベクトルを決定しますが, クエリベクトルはデコーダセクション内で決定されます。
</p>
<p>
エンコーダモジュールとデコーダモジュールを組み合わせると, 図12.20に示すモデルのアーキテクチャが得られます。モデルは, 入力文と出力文のペアを使用してトレーニングできます。
</p>
<p>
図12.20。シーケンストゥーシーケンストランスフォーマーの概略図。図をすっきりさせるために, 入力トークンは1つのボックスとしてまとめて表示されており, 出力トークンも同様です。位置エンコーディングベクトルは, エンコーダセクションとデコーダセクションの両方の入力トークンに追加されます。エンコーダの各層は図12.9に示す構造に対応し, 各クロスアテンション層は図12.19に示す形式になります。
</p>
<h3>12.3.5 Large language models</h3>
<p>
機械学習の分野における最近の最も重要な開発は, 大規模言語モデルまたはLLMとして知られる, 自然言語処理用の非常に大規模なトランスフォーマーベースのニューラルネットワークの作成です。ここでの’大規模’とは, ネットワーク内の重みとバイアスのパラメーターの数を指し, 本稿執筆時点では最大約1兆(1012) に達する可能性があります。 このようなモデルのトレーニングには費用がかかり, モデルを構築する動機はその並外れた能力にあります。
</p>
<p>
大量のデータセットが利用可能になったことに加えて, GPU(グラフィックスプロセッシングユニット)や同様のプロセッサが大規模なクラスターに密接に結合され, 高速なインターコネクトと多くのオンボードメモリが装備された大量並列トレーニングハードウェアの出現により, ますます大きなモデルのトレーニングが容易になっています。トランスフォーマーアーキテクチャは, このようなハードウェアを非常に効率的に使用できるため, これらのモデルの開発において重要な役割を果たしています。トレーニングデータセットのサイズを増やすことと, モデルパラメータの数を同等に増やすことが, しばしば, アーキテクチャの改善やより多くのドメイン知識を組み込む他の方法を上回る性能向上につながります(Sutton, 2019; Kaplan et al., 2020)。例えば, GPTシリーズのモデル(Radford ef al., 2019; Brown et al., 2020; OpenAl, 2023)の印象的な性能向上は, 世代を重ねるごとに規模が拡大したことによるものが主でした。このような性能向上は, Mooreの法則の新しい種類を推進しており, 最先端の機械学習モデルをトレーニングするために必要な計算操作の数は, 2012年頃以降, 約3.4か月の倍増時間で指数関数的に増加しています。
</p>
<p>
初期の言語モデルは教師あり学習を使用してトレーニングされました。たとえば, 翻訳システムを構築する場合, トレーニングセットは2つの言語の一致する文のペアで構成されます。ただし, 教師あり学習の主な制限は, 通常, ラベル付きの例を提供するにはデータを人間が厳選する必要があり, これによって利用可能なデータの量が大幅に制限されるため, 合理的なパフォーマンスを達成するのに特徴量エンジニアリングやアーキテクチャ制約などの帰納的バイアスを多用する必要があることです。
</p>
<p>
大規模な言語モデルは, 代わりに, コンピューターコードなどの他のトークンシーケンスとともに, テキストの非常に大規模なデータセットに対する自己教師あり学習によってトレーニングされます。条件付き確率分布を学習するために, 前のシーケンスを入力として, 各トークンがラベル付きターゲットサンプルとして機能するトークンシーケンスでデコーダートランスフォーマーをトレーニングする方法を見てきました。この’自己ラベル付け’により, 利用可能なトレーニングデータの量が大幅に増加するため, 多数のパラメータを持つディープニューラルネットワークの利用が可能になります。
</p>
<p>
この自己教師あり学習の使用により, 最初にラベルなしデータを使用して大規模なモデルが事前トレーニングされ, その後, はるかに小さいラベル付きデータのセットに基づいて教師あり学習を使用して微調整されるというパラダイムシフトが生じました。これは事実上転移学習の一種であり, 同じ事前トレーニング済みモデルを複数の’下流’アプリケーションに使用できます。 特定のタスクに合わせて後で微調整できる幅広い機能を備えたモデルは, 基礎モデルと呼ばれます (Bommasani et al., 2021)。
</p>
<p>
微調整は, ネットワークの出力に追加レイヤーを加えるか, 最後のいくつかのレイヤーを新しいパラメーターに置き換えてから, ラベル付きデータを使用してこれらの最終レイヤーをトレーニングすることによって実行できます。微調整段階では, メインモデルの重みとバイアスを変更しないことも, 小さなレベルの適応を許可することもできます。通常, 微調整のコストは事前トレーニングのコストと比較して少額です。
</p>
<p>
非常に効率的なファインチューニングのアプローチの1つは, 低ランク適応またはLORA(Huら, 2021)と呼ばれます。このアプローチは, 過剰パラメータ化されたトレーニング済みモデルが, ファインチューニングに関して固有の次元数が低く, ファインチューニング中のモデルパラメータの変化が, モデル内の学習可能なパラメータの総数よりもはるかに小さい次元数の多様体上にあることを示す結果に着想を得ています(Aghajanyan, Zettlemoyer, Gupta, 2020)。LoRaは, これを利用して, 元のモデルの重みを凍結し, トランスフォーマーの各層に低ランク製品の形で追加の学習可能な重み行列を追加します。通常, 注意レイヤーの重みのみが変更され, MLPレイヤーの重みは固定されます。次に, D×Dの次元を持つ重み行列W0を考えます。これは, 複数の注意ヘッドからの行列が1つの行列として扱われるクエリ, キー, または値の行列を表す場合があります。次に, 2つの行列AとBの積で定義される並列の重みセットをD×RおよびR×Dの次元で導入します。これは, 図12.21に示すように, 出力XW0 + XABを生成するレイヤーを生成します。追加の重み行列ABのパラメータ数は, 元の重み行列W0のD2パラメータに比べて2RDであるため, R << Dの場合, ファインチューニング中に適応する必要があるパラメータ数は, 元のトランスフォーマーのパラメータ数のはるかに小さいことになります。実際には, これにより, トレーニングする必要のあるパラメータ数が10,000分の1に減少することができます。ファインチューニングが完了したら, 追加の重みを元の重み行列に追加して, 新しい重み行列を作成できます。
\[
\tag{12.36}
\]
</p>
<p>
そのため, 更新されたモデルのサイズは元のモデルと同じであるため, 推論中に元のモデルを実行する場合と比較して追加の計算オーバーヘッドは発生しません。
</p>
<p>
図12.21。事前訓練されたトランスフォーマーのアテンション層の1つからの重み行列W0を示す低ランク適応の概略図。行列AおよびBによって与えられる追加の重みは微調整中に適応され, その後の推論のためにその積ABが元の行列に追加されます。
</p>
<p>
言語モデルが大規模かつ強力になるにつれて, 微調整の必要性が減り, 生成言語モデルはテキストベースの対話だけで幅広いタスクを解決できるようになりました。 たとえば, テキスト文字列の場合, 
\[
English:　the　cat　sat　on　the　mat.　French:
\]
が入力シーケンスとして与えられると, 自己回帰言語モデルは, (停止)トークンが生成されるまで後続のトークンを生成し続けることができ, 新しく生成されたトークンはフランス語の翻訳を表します。このモデルは翻訳を行うために特別にトレーニングされたわけではありませんが, 複数の言語を含む膨大なデータコーパスでトレーニングされた結果, 翻訳を行うようになったことに注意してください。
</p>
<p>
ユーザーは自然言語対話を使用してこのようなモデルを操作できるため, 幅広い視聴者にとって非常にアクセスしやすいものになります。ユーザーエクスペリエンスと生成された出力の品質を向上させるために, RLHFによる人間のフィードバックによる強化学習などの方法を使用して, 生成された出力の人による評価を通じて大規模な言語モデルを微調整する技術が開発されました(Christiano et al. 2017)。このような手法は, 印象的に使いやすい会話インターフェイスを備えた大規模な言語モデルの作成に役立ち, 特に注目すべきはChatGPTと呼ばれるOpenAIのシステムです。
</p>
<p>
ユーザーが入力したトークンのシーケンスはプロンプトと呼ばれます。たとえば, それは物語の最初の言葉から構成される場合があり, モデルはそれを完成させる必要があります。また, 質問から構成される場合もあり, モデルは答えを提供する必要があります。異なるプロンプトを使用することで, 同じトレーニング済みニューラルネットワークは, コンピュータコードを生成したり, リクエストに応じて韻を踏んだ詩を書いたりするなど, 幅広いタスクを解決することができるようになります。モデルのパフォーマンスは, プロンプトの形式に依存するため, プロンプトエンジニアリング(Liu et al., 2021)という新しい分野が生まれ, ダウンストリームタスクの高品質な出力をもたらす良好なプロンプトの形式を設計することを目的としています。ユーザープロンプトを言語モデルに送信する前に, プレフィックスプロンプトと呼ばれる追加のトークンシーケンスをユーザープロンプトの前に付け加えて, 出力の形式を変更することによって, モデルの動作を変更することもできます。たとえば, プレプロンプトには, ネットワークが出力に不適切な言葉を含めないようにするための標準英語で表現された指示が含まれる場合があります。
</p>
<p>
これにより, モデルのパラメーターを調整することなく, プロンプト内でいくつかの例を提供するだけで, モデルは新しいタスクを解決できるようになります。 これは少数ショット学習の例です。
</p>
<p>
GPT-4などの現在の最先端モデルは非常に強力になっており, 汎用人工知能の最初の兆候と言われている顕著な特性を示しており(Bubeckら, 2023), 新たな技術革新の波を引き起こしています。さらに, これらのモデルの機能は驚くべきペースで向上し続けています。
</p>
<h2>12.4 Multimodal Transformers</h2>
<p>
トランスフォーマーは当初, 逐次言語データを処理するためのリカレントネットワークの代替として開発されましたが, 現在では深層学習のほぼすべての分野で普及しています。これらは, 等分散性や局所性について強い仮定を置く畳み込みネットワークなどとは対照的に, 入力データに関する仮定をほとんど行わないため, 汎用モデルであることが証明されています。 トランスフォーマーはその汎用性により, テキスト, 画像, ビデオ, 点群, 音声データなどのさまざまなモダリティの最先端となっており, これらの各ドメイン内の識別アプリケーションと生成アプリケーションの両方に使用されています。 。 変圧器層のコア アーキテクチャは, 時間の経過とアプリケーション全体の両方で, 比較的一定のままです。 したがって, 自然言語以外の分野でトランスフォーマーの使用を可能にした主要な革新は, 主に入力と出力の表現とエンコードに焦点を当ててきました。
</p>
<p>
多くの異なる種類のデータを処理できる単一のアーキテクチャの大きな利点の1つは, マルチモーダルな計算が比較的簡単になることです。この文脈では, マルチモーダルとは, 入力または出力, またはその両方で2つ以上の異なるタイプのデータを組み合わせるアプリケーションを指します。たとえば, テキストプロンプトから画像を生成したり, カメラ, レーダー, マイクなどの複数のセンサーからの情報を組み合わせられるロボットを設計したりすることができます。注意すべき重要な点は, 入力をトークン化し, 出力トークンをデコードできれば, トランスフォーマーを使用できる可能性が高いということです。
</p>
<h3>12.4.1 Vision transformers</h3>
<p>
トランスフォーマーはコンピュータービジョンに適用されて大きな成功を収め, 多くのタスクで最先端のパフォーマンスを達成しました。識別タスクに最も一般的に選択されるのは標準のトランスフォーマーエンコーダーであり, 視覚領域におけるこのアプローチはビジョントランスフォーマー, またはViTとして知られています(Dosovitskiy et al., 2020)。トランスフォーマーを使用する場合, 入力画像をトークンに変換する方法を決定する必要があります。最も簡単な選択は, 線形投影に従って各ピクセルをトークンとして使用することです。ただし, 標準のトランスフォーマー実装に必要なメモリは, 入力トークンの数に応じて二次関数的に増加するため, このアプローチは一般に実行不可能です。代わりに, トークン化に対する最も一般的なアプローチは, イメージを同じサイズのパッチの集合に分割することです。画像の寸法がx∈RH×W×Cであるとします。ここで, HとWはピクセル単位の画像の高さと幅, Cはチャネル数です(通常, R, G, Bの色ではC = 3)。各画像はサイズP×P (P = 16 が一般的な選択)の重なり合わないパッチに分割され, その後1次元ベクトルに’平坦化’され, xp∈RN×(P2C)という表現が得られます。ここで, N = HW/P2 は1つのイメージのパッチの総数です。ViTアーキテクチャを図 12.22 に示します。
</p>
<p>
図12.22。分類タスク用のビジョントランスフォーマーアーキテクチャの図。ここでは, 学習可能な(クラス)トークンが追加の入力として含まれており, 関連する出力は, LSMで示されるソフトマックスアクティベーションを備えた線形層によって変換され, 最終的なクラスベクトル出力cが得られます。
</p>
<p>
トークン化のもう1つのアプローチは, 小規模な畳み込みニューラルネットワーク(CNN)を通じて画像をフィードすることです。これにより, イメージをダウンサンプリングして, ネットワーク出力の1つでそれぞれ表される管理可能な数のトークンを得ることができます。たとえば, 一般的なResNet18エンコーダアーキテクチャは, 画像を高さと幅の両方の寸法で8分の1 にダウンサンプリングし, トークンの数がピクセルの64分の1になります。
</p>
<p>
トークン内の位置情報をエンコードする方法も必要です。画像パッチの2次元位置情報をエンコードする明示的な位置エンベディングを構築することは可能ですが, 実際にはこれによってパフォーマンスが向上するわけではないため, 学習された位置エンベディングをそのまま使用するのが最も一般的です。自然言語に使用されるトランスフォーマーとは対照的に, ビジョン トランスフォーマーは通常, 固定数のトークンを入力として受け取ります。これにより, 学習された位置エンコーディングが異なるサイズの入力に一般化されないという問題が回避されます。
</p>
<p>
ビジョントランスフォーマーのアーキテクチャ設計は, CNNとは大きく異なります。CNNモデルには強力な帰納バイアスがベイクされていますが, ビジョントランスフォーマーの唯一の2次元帰納バイアスは, 入力のトークン化に使用されるパッチによるものです。したがって, トランスフォーマーは画像の幾何学的特性を最初から学習する必要があるため, 一般的に同等のCNNよりも多くのトレーニングデータを必要とします。ただし, 入力の構造について強い仮定がないため, 多くの場合, トランスフォーマーはより高い精度に収束できます。これは, 帰納バイアスとトレーニングデータのスケールの間のトレードオフを示す別の説明になります(Sutton, 2019)。
</p>
<p>
言語ドメインでは, テキストを合成するための自己回帰生成モデルとしてトランスフォーマーを使用したときに最も印象的な結果が得られました。したがって, トランスフォーマーを使用してリアルな画像を合成できないかという疑問が生じるのは自然なことです。自然言語は本質的に逐次的なものであるため, 自己回帰フレームワークにうまく適合しますが, 画像にはピクセルの自然な順序がないため, 自己回帰的にデコードすることがそれほど直感的ではありません。ただし, 最初に変数の順序を定義すれば, どの分布も条件の積に分解できます。したがって, 順序付き変数x1, ..., xNにわたる結合分布は次のように書くことができます。
\[
\tag[12.37}
\]
</p>
<p>
この因数分解は完全に一般的であり, 個々の条件付き分布p(xn | x1, ..., xn—1)の形式に制限はありません。
</p>
<p>
画像の場合, n番目のピクセルをRGB値の3次元ベクトルとして表すためにxnを選択できます。次に, ピクセルの順序を決定する必要があります。図12.23に示すように, 広く使用されている選択肢の1つはラスタースキャンと呼ばれます。ラスタースキャン順序に基づいて, 自己回帰モデルを使用して生成される画像の概略図を図12.24に示します。
</p>
<p>
図12.23。2次元画像内のピクセルの特定の線形順序を定義するラスタースキャンの図。
</p>
<p>
図12.24。自己回帰モデルから画像をサンプリングする方法を示した図。最初のピクセルは周辺分布p(x11)からサンプリングされ, 2番目のピクセルは条件付き分布p(x12|x11)からサンプリングされ, 完全な画像が得られるまでラスタースキャン順に続きます。
</p>
<p>
画像の自己回帰生成モデルの使用は, トランスフォーマーの導入よりも前から行われていたことに注意してください。たとえば, PixelCNN (Oord et al., 2016)とPixel-RNN (Oord, Kalchbrenner, および Kavukcuoglu, 2016)は, 12.37の右辺に対応する項によって各ピクセルに定義された条件付き独立性を維持する, 特注のマスクされた畳み込み層を使用しました。
</p>
<p>
連続値を使用した画像の表現は, 識別タスクでうまく機能します。ただし, 離散表現を使用すると, 画像生成の方がはるかに優れた結果が得られます。負の対数尤度関数が二乗和誤差関数であるガウス分布など, 最尤法によって学習された連続条件付き分布は, トレーニングデータの平均を学習する傾向があり, 画像がぼやけてしまいます。逆に, 離散分布はマルチモダリティを簡単に処理できます。たとえば, (12.37)の条件付き分布p(xn | x1, ..., xn-1)の1つは, ピクセルが黒か白のいずれかであることを学習する可能性がありますが, 回帰モデルはピクセルが灰色でなければならないことを学習する可能性があります。
</p>
<p>
ただし, 個別のスペースを扱うには課題も伴います。画像ピクセルのR, G, B値は通常, 少なくとも8ビットの精度で表されるため, 各ピクセルは224 ～ 16Mの可能な値を持ちます。このような高次元空間にわたる条件付きソフトマックス分布を学習することは不可能です。
</p>
<p>
高次元の問題に対処する1つの方法は, データ圧縮の一種とみなすことができるベクトル量子化の技術を使用することです。 たとえば画像ピクセルを表す, それぞれ次元Dのデータベクトルx1, ..., xNの集合があると仮定します。次に, K個のコードブックベクトルの集合C = c1, ..., cKも導入します。次元Dで, 通常はK << Dです。ここで, 何らかの類似性メトリック(通常はユークリッド距離)に従って, 各データベクトルを最も近いコードブック ベクトルで近似します。
\[
\tag{12.38}
\]
</p>
<p>
K個のコードブックベクトルがあるため, 各xnをワンホットエンコードされたK次元ベクトルで表すことができます。また, Kの値を選択できるため, Kの値を大きくすることでデータをより正確に表現するか, Kの値を小さくして圧縮率を高くするかの間のトレードオフを制御できます。
</p>
<p>
したがって, 元の画像ピクセルを取得して, それらを低次元のコードブック空間にマッピングできます。次に, 自己回帰変換器をトレーニングしてコードブックベクトルのシーケンスを生成し, 各コードブックインデックスkを対応するD次元コードブックベクトルckに置き換えることによって, このシーケンスを元の画像空間にマッピングし直すことができます。
</p>
<p>
自己回帰変換は, ImageGPT(Chen, Radford, et al., 2020)の画像に初めて適用されました。ここで, 各ピクセルは, 3次元カラーコードブックベクトルの離散集合の1つとして扱われ, それぞれが色空間のk-meansクラスタリングのクラスターに対応します。したがって, ワンホットエンコーディングは, 言語トークンに似た離散トークンを提供し, 次のトークンの分類目標を使用して, 言語モデルと同じ方法でトランスフォーマーをトレーニングできるようにします。これは, やはり言語モデリングと同様の方法で, その後の微調整のための表現学習の強力な目標です。
</p>
<p>
ただし, 個々のピクセルをトークンとして直接使用すると, ピクセルごとに前方パスが必要になるため, 計算コストが高くなる可能性があります。これは, トレーニングと推論の両方が画像解像度に合わせてうまくスケールできないことを意味します。 また, 個々のピクセルを入力として使用するということは, 後のラスタースキャンでピクセルをデコードするときに, 適切なコンテキスト長を与えるために低解像度の画像を使用する必要があることを意味します。ViTモデルで見たように, ピクセルの代わりに画像のパッチをトークンとして使用することをお勧めします。これにより, トークンの数が大幅に減り, 高解像度の画像での作業が容易になります。前と同様, 条件付き分布には潜在的な多峰性があるため, トークン値の離散空間を扱う必要があります。繰り返しますが, これにより次元性の問題が生じます。次元性はパッチ内のピクセル数に対して指数関数的であるため, パッチでは個々のピクセルよりもはるかに深刻になります。たとえば, 白と黒を表すピクセルトークンが2つだけあり, サイズが16×16のパッチがある場合でも, サイズ2256 ～ 1077のパッチトークンの辞書が存在します。
</p>
<p>
次元の課題に対処するために, もう一度ベクトル量子化に目を向けます。コードブックベクトルは, K-平均法などの単純なクラスタリングアルゴリズム, または完全畳み込みネットワーク(ord, Vinyals, および Kavukcuoglu, 2017; Esser, Rombach, および Ommer, 2020)などのより高度な方法を使用して, 画像パッチのデータセットから学習できます。またはビジョントランスフォーマー(Yu et al., 2021)さえあります。各パッチをコードの離散集合にマッピングし, 再度マッピングする方法を学習する際の問題の1つは, ベクトル量子化が微分不可能な操作であることです。幸いなことに, ストレートスルー勾配推定(Bengio, Léonard, および Courville, 2013)と呼ばれる手法を使用できます。これは, バックプロパゲーション中に非微分関数を通じて勾配をコピーするだけの単純な近似です。
</p>
<p>
画像を生成するための自己回帰変換器の使用は, ビデオをこれらのベクトル量子化トークンの1つの長いシーケンスとして扱うことによってビデオに拡張できます(Rakhimov et al., 2020; Yan et al., 2021; Hu et al., 2023)。
</p>
<h3>12.4.3 Audio data</h3>
<p>
次に, オーディオデータへのトランスフォーマーの適用について見ていきます。音は通常, 一定の時間間隔で気圧の振幅を測定して得られる波形として保存されます。この波形は深層学習モデルへの入力として直接使用できますが, 実際には, メルスペクトログラムに前処理する方がより効果的です。これは, 列がタイムステップを表し, 行が周波数に対応する行列です。周波数帯域は, 連続する周波数間に等しい知覚差を与えるために主観的な評価によって選択された標準的な規則に従っています (‘メル’という言葉はメロディーに由来します)。メルスペクトログラムの例を図 12.25 に示します。
</p>
<p>
図12.25。ザトウクジラの鳴き声のメルスペクトログラムの例。
</p>
<p>
オーディオドメインにおけるトランスフォーマーのアプリケーションの1つは, オーディオのセグメントを多数の事前定義されたカテゴリの1つに割り当てる分類です。たとえば, AudioSerデータセット(Gemmeke et al., 2017)は広く使用されているベンチマークです。’車’, ‘動物’, ‘笑い’などのクラスが含まれています。トランスフォーマーが開発されるまで, 音声分類の最先端のアプローチは, 画像として扱われ, 畳み込みニューラル ネットワーク (CNN) への入力として使用されるメルスペクトログラムに基づいていました。ただし, CNNはローカルな関係を理解するのには優れていますが, 1つの欠点は, 音声を処理する際に重要になる可能性がある長距離の依存関係に苦戦することです。
</p>
<p>
自然言語処理の最先端技術としてトランスフォーマーがRNNに取って代わったのと同様に, ランスフォーマーは音声分類などのタスクでもCNNに取って代わるようになりました。たとえば, 図12.18に示すように, 言語と視覚の両方に使用されるものと同じ構造のトランスエンコーダーモデルを使用して, オーディオ入力のクラスを予測できます(Gong, Chung, および Glass, 2021)。ここでは, メルスペクトログラムは画像として表示され, その後トークン化されます。これは, ビジョントランスフォーマーと同様の方法で画像をパッチに分割することによって行われます。重要な隣接関係が失われないように, 場合によっては一部のオーバーラップを追加します。次に, 各パッチはフラット化され, この場合は長さ256の 1次元配列に変換されます。次に, 一意の位置エンコーディングが各トークンに追加され, 特定の(クラス)トークンが追加され, トランスフォーマーエンコーダーを介してトークンが供給されます。最後のトランスフォーマー層からの(クラス)入力トークンに対応する出力トークンは, 線形層とそれに続くソフトマックス活性化関数を使用してデコードでき, クロスエントロピー損失を使用してモデル全体をエンドツーエンドでトレーニングできます。
</p>
<h3>12.4.4 Text-to-speech</h3>
<p>
深層学習, より具体的にはトランスフォーマーアーキテクチャがオーディオ領域で革命をもたらしたタスクは分類だけではありません。特定の話者の声を模倣する音声合成におけるトランスフォーマーの成功は, トランスフォーマーの多用途性を示すもう1つの例であり, このタスクへのトランスフォーマーの適用は, 新しい状況でトランスフォーマーを適用する方法についての有益なケーススタディです。
</p>
<p>
テキストの特定の一節に対応する音声を生成することは, テキスト音声合成として知られています。より伝統的なアプローチは, 特定の話者から音声の録音を収集し, 対応する書き起こされたテキストから, おそらくメルスペクトログラムの形式で音声出力を予測する教師あり回帰モデルをトレーニングすることです。推論中, 音声を合成したいテキストが入力として提示され, その結果のメルスペクトログラム出力は固定マッピングであるため, デコードしてオーディオ波形に戻すことができます。
</p>
<p>
ただし, このアプローチにはいくつかの大きな欠点があります。まず, たとえば音素として知られるサブワードコンポーネントを使用して音声を低レベルで予測する場合, 結果の文を滑らかに聞こえるようにするには, より大きなコンテキストが必要になります。ただし, より長いセグメントを予測すると, 可能な入力の空間が大幅に増大し, 適切な一般化を達成するには, 実行不可能な量のトレーニングデータが必要になる可能性があります。第2に, このアプローチでは話者間で知識が伝達されないため, 新しい話者ごとに大量のデータが必要になります。最後に, この問題は実際には生成モデリングタスクです。特定の話者とテキストのペアに対して複数の正しい音声出力があるため, 平均が目標値を超える傾向があるため, 回帰は適切ではない可能性があります。
</p>
<p>
代わりに, 音声データを自然言語と同じ方法で扱い, テキスト読み上げを条件付き言語モデリングタスクとしてフレーム化すれば, テキストベースの大規模言語モデルとほぼ同じ方法でモデルをトレーニングできるはずです。対処する必要がある主な実装の詳細が2つあります。1つ目はトレーニングデータをトークン化して予測をデコードする方法, 2つ目は話者の音声に基づいてモデルを調整する方法です。
</p>
<p>
トランスフォーマーと言語モデリング技術を利用したテキスト音声合成へのアプローチの1つは, Vall-E(Wang et al., 2023)です。 新しいテキストは, その人のわずか数秒のサンプル音声を使用して, 新しい話者の声の音声にマッピングできます。音声データは, 学習された辞書またはベクトル量子化を使用して取得されたコードブックから一連の離散トークンに変換されます。これらのトークンは, 自然言語ドメインからのワンホットエンコードされたトークンに類似していると考えることができます。 入力はテキストの一節からのテキストトークンで構成され, トレーニングのターゲット出力は対応する音声トークンで構成されます。図12.26に示すように, 同じ話者による無関係な音声の短いセグメントからの追加の音声トークンが入力テキストトークンに追加されます。多くの異なる話者からの例を含めることにより, システムは, 追加の音声入力トークンによって表される音声を模倣しながら, テキストの一節を読み上げることを学習できます。トレーニングが完了すると, システムに新しいテキストと, 新しい話者からキャプチャされた音声の短いセグメントからのオーディオトークンを提示できます。結果として得られる出力トークンは, トレーニング中に使用したものと同じコードブックを使用してデコードして, 音声波形を作成できます。これにより, システムは, 新しい話者の音声の入力テキストに対応する音声を合成できます。
</p>
<p>
図12.26。Vall-Eの高レベルのアーキテクチャを示す図。変換モデルへの入力は, 合成音声にどのような単語を含めるかをモデルに指示する標準テキストトークンと, 話者のスタイルとトーン情報を決定する音響プロンプトトークンで構成されます。サンプリングされたモデル出力トークンは, 学習されたデコーダーを使用してデコードされて音声に戻されます。簡単にするために, 位置エンコーディングと線形投影は示されていません。
</p>
<h3>12.4.5 Vision and language transformers</h3>
<p>
テキスト, オーディオ, 画像の個別のトークンを生成する方法を説明しました。そのため, あるモダリティの入力トークンと別のモダリティの出力トークンを使用してモデルをトレーニングできるかどうか, または 入力または出力, またはその両方に対するさまざまなモダリティの組み合わせ。これは最も広く研究されている例であるため, テキストデータとビジョンデータの組み合わせに焦点を当てますが, 原理的には, ここで説明するアプローチは入力モダリティと出力モダリティの他の組み合わせにも適用できます。
</p>
<p>
最初の要件は, トレーニング用に大規模なデータセットがあることです。LAION-400Mデータセット(Schuhmann e7 al., 2021)は, ImageNetが深層画像分類モデルの開発に不可欠であったのとほぼ同じ方法で, テキストから画像への生成と画像からテキストへのキャプション付けの研究を大幅に加速しました。テキストから画像への生成は, 実際にはこれまで見てきた無条件画像生成とよく似ていますが, モデルが生成プロセスを条件付けるテキスト情報を入力として受け取ることもできる点が異なります。トランスフォーマーを使用する場合, 各画像トークンをデコードするときに追加入力としてテキストトークンを提供するだけで済むため, これは簡単です。
</p>
<p>
このアプローチは, ターゲットトークンが言語トークンではなく離散画像トークンである点を除き, テキストから画像への問題を, 機械翻訳などのシーケンスからシーケンスへの言語モデリング問題として扱うものとみなすこともできます。したがって, 図12.20に示すように, 完全なエンコーダ/デコーダ変換モデルを選択することが理にかなっています。Xは入力テキスト トークンに対応し, Yは出力画像トークンに対応します。これは, Parti(Yu et al, 2022)と呼ばれるモデルで採用されたアプローチであり, トランスフォーマーは200億のパラメーターにスケールされ, モデルサイズの増加に伴って一貫したパフォーマンスの向上が見られます。
</p>
<p>
事前にトレーニングされた言語モデルを使用し, 視覚データも入力として受け入れられるようにそれらを変更または微調整することについても多くの研究が行われています(Alayrac er al., 2022: Li er al., 2022)。これらのアプローチは主に, 連続値の画像トークンとともにオーダーメイドのアーキテクチャを使用するため, 視覚データの生成にも不自然です。さらに, オーディオ トークンなどの新しいモダリティを含めたい場合, それらを直接使用することはできません。これはマルチモダリティへの一歩ですが, 理想的にはテキストと画像の両方のトークンを入力と出力の両方として使用したいと考えています。最も簡単なアプローチは, すべてを自然言語であるかのようにトークンのシーケンスとして扱うことですが, 言語トークン辞書と画像トークンコードブックを連結した辞書を使用します。これにより, オーディオおよびビジュアルデータのあらゆるストリームを単なるトークンのシーケンスとして扱うことができます。
</p>
<p>
CMB(Aghajanyan er al., 2022)とCM3Leon (Yu et al., 2023)では, オンラインソースから取得された画像とテキストデータを含むHTMLドキュメントを学習するために, 言語モデリングの変種が使用されています。この大量のトレーニングデータをスケーラブルなアーキテクチャと組み合わせることで, モデルは非常に強力になりました。また, トレーニングの多様性により, モデルは非常に柔軟です。このようなモデルは, テキストから画像を生成したり, 画像にキャプションを付けたり, 画像を編集したり, テキストを補完したり, 通常の言語モデルができることを含め, 多くのタスクを完了することができます。CM3Leonモデルがいくつかの異なるタスクのインスタンスを完了する様子が, 図12.27に示されています。
</p>
<p>
図 12.27。テキストと画像の結合空間でさまざまなタスクを実行するCM3Leonモデルの例。
</p>
    </body>
</html>