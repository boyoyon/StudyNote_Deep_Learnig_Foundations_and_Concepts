<!doctype html>
<html lang="ja">
    <head>
        <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
        <meta charset="utf-8" />
        <title>12章</title>
        <style type="text/css">
            p
            {
                padding-left: 2em;
            }
            .margin-large
            {
                margin-left: 30px;
            }
           .margin-abstract {
               margin-left: 60px; /* 左マージンを広くする */
               margin-right: 60px; /* 右マージンを広くする */
           }
        </style>
    <style>
        .two-columns {
            display: flex;
            flex-direction: row;
            gap: 20px; /* 列間のスペース */
        }
        .column {
            flex: 1; /* 各列が均等に幅を取る */
        }
    </style>
<style>
.three-columns {
  display: flex;
  gap: 10px; /* 列間の余白を設定 */
}
.column {
  flex: 1; /* 各列の幅を均等にする */
  padding: 10px; /* 内側の余白を設定 */
}
</style>
    <style>
        .styleRef { 
            text-indent: -40px; /* 最初の行の字下げを逆方向に */
            margin-left: 10px; /* 2行目以降の字下げを調整 */
            ul {
                  list-style-type: none; /* 箇条書き記号を非表示 */
                  padding-left: 40px; /* 全体の左余白 */
            }
            li {
            }
        }
    </style>
    <style>
        .styleBullet { 
            text-indent: -20px; /* 最初の行の字下げを逆方向に */
            margin-left: 30px; /* 2行目以降の字下げを調整 */
            ul {
                  list-style-type: none; /* 箇条書き記号を非表示 */
                  padding-left: 0px; /* 全体の左余白 */
            }
            li {
            }
        }
    </style>
    <style>
            ol
            {
                margin-left: 30px;
            }
            ul
            {
                margin-left: 30px;
            }
    </style>
    <style>
        .highlight {
            color: red; /* 好きな色に変更してください */
        }
    </style>
    </head>
    <body>
        <h1><center>12章 トランスフォーマー</center></h1>
<p>
トランスフォーマーは, 深層学習における最も重要な発展の1つです。これらは, アテンション(注意)と呼ばれる処理概念に基づいており, ネットワークが入力値に依存する重み係数を使用して, 異なる入力に異なる重みを与えることができるため, シーケンシャルなデータやその他の形式のデータに関連する強力な帰納バイアスを捕捉できます。
</p>
<p>
これらのモデルは, ある表現空間内のベクトルの集合を, 新しい空間内の同じ次元を持つ対応するベクトルの集合に変換するため, トランスフォーマー 変換器 として知られています。変換の目標は, 新しい空間が, 下流のタスクの解決により適した,
より豊富な内部表現を持つようになることです。トランスフォーマーへの入力は, 構造化されていないベクトルの集合, 順序付けられたシーケンス, またはより一般的な表現の形式を取ることができるため, トランスフォーマーに幅広い適用性が与えられます。
</p>
<p>
トランスフォーマーはもともと自然言語処理, つまりNLP 自然 言語とは英語や北京語などの言語です のコンテキストで導入され, リカレントニューラルネットワーク RNN に基づく以前の最先端のアプローチを大幅に上回りました。その後, トランスフォーマーが他の多くの分野で優れた結果を達成することが判明しました。たとえば, ビジョントランスフォーマーは, 画像処理タスクにおいてCNN よりも優れたパフォーマンスを発揮することがよくありますが, テキスト, 画像, オーディオ, ビデオなどの複数の種類のデータを組み合わせるマルチモーダルトランスフォーマーは, 最も強力な深層学習モデルの1 つです。
</p>
<p>
トランスフォーマーの主な利点の1 つは, 転移学習が非常に効果的であるため, トランスフォーマーモデルを大量のデータでトレーニングでき, トレーニングされたモデルを何らかの形式の微調整を使用して多くの下流タスクに適用できることです。 複数の異なるタスクを解決するために後で適合させることができる大規模モデルは, 基礎モデルとして知られています。さらに, トランスフォーマーは, ラベルなしのデータを使用して自己教師ありの方法でトレーニングできます。これは, トランスフォーマーがインターネットやその他のソースから入手可能な膨大な量のテキストを利用できるため, 言語モデルの場合に特に効果的です。スケーリング仮説は, 学習可能なパラメーターの数で測定されるモデルのスケールを大きくし, それに見合った大規模なデータセットでトレーニングするだけで, アーキテクチャを変更しなくても, パフォーマンスの大幅な向上が達成できると主張します。さらに, このトランスフォーマーは, グラフィックプロセッシングユニット GPU などの大規模並列処理ハードウェアに特に適しており, \(10 ^{12}\) 個のオーダーのパラメーターを持つ非常に大規模なニューラルネットワーク言語モデルを適切な時間内でトレーニングできるようになります。このようなモデルは並外れた能力を備えており, 汎用人工知能の初期の兆候として説明されている創発的な特性の明確な兆候を示します Bubeck et al., 2023 。
</p>
<p>
トランスフォーマーのアーキテクチャは, 複数の異なるコンポーネントが連携して動作し, さまざまな設計の選択が恣意的に行われるように見えるため, 初心者にとっては複雑で, 気が遠くなるようなものにさえ思えるかもしれません。したがって, この章では, トランスフォーマーの背後にあるすべての重要なアイデアを段階的に包括的に紹介し, さまざまな要素の設計を動機付けるための明確な直観を提供することを目的としています。最初にトランスフォーマーのアーキテクチャについて説明し,
次に自然言語処理に焦点を当ててから, 他のアプリケーションドメインを検討します。
</p>
<h2>12.1 アテンション</h2>
<p>
トランスフォーマーを支える基本的な概念は アテンションです。これは元々, 機械翻訳用の RNN の拡張機能として開発されました Bahdanau , Cho , およびBengio , 2014 。しかし, Vaswani ら 2017 は後に, 再帰構造を排除し, 代わりにアテンションメカニズムのみに焦点を当てることでパフォーマンスが大幅に向上することを示しました。現在, アテンションに基づくトランスフォーマーは, ほぼすべてのアプリケーションでRNN に完全に取って代わりました。
</p>
<p>
アテンションはより広範囲に適用可能ですが, 例として自然言語を使用してアテンションの利用を動機付けします。次の 2つの文を考えてみましょう。
</p>
<p class="margin^large">
● I swam across the river to get to the other <strong><span class="highlight">bank</span></strong>.<br>
　私は向こう<strong><span class="highlight">岸</span></strong>に行くために川を泳いで渡った。<br>
　<br>
● I walked across the road to get cash from the <strong><span class="highlight">bank</span></strong>.<br>
　私は<strong><span class="highlight">銀行</span></strong>から現金を受け取るために道路を渡って歩いた。
</p>
<p>
ここで, bank という単語は2 つの文で異なる意味を持っています。ただし, これはシーケンス内の他の単語によって提供されるコンテキストを調べることによってのみ検出できます。また, bank の解釈を決定する際に, 一部の単語が他の単語より重要であることもわかります。最初の文では, swam と river という単語が, bank が川の岸辺を指すことを最も強く示していますが,2
番目の文では, cash という単語が bank が金融機関を指すことを強く示しています。 bank の適切な解釈を決定するには, そのような文を処理するニューラルネットワークがシーケンスの残りの部分の特定の単語に注意を払う必要がある, つまり, より多くの単語に依存する必要があることがわかります。このアテンションの概念を図 12.1 に示します。
</p>
<center><img src="images/fig12_1.png"></center>
<p class="margin-large">
図12.1 。 bank という単語の解釈が river と swam という単語によって影響を受けるアテンションの概略図。各線の太さはその影響の強さを示しています。
</p>
<p>
さらに, より注目すべき特定の位置は入力シーケンス自体に依存していることもわかります。最初の文では重要なのは2 番目と5 番目の単語ですが, 2 番目の文では8 番目の単語です。標準的なニューラルネットワークでは, 異なる入力は, それらの入力を乗算する重みの値に応じて, 異なる範囲で出力に影響を与えます。ただし, ネットワークがトレーニングされると, それらの重みとそれに関連する入力は固定されます。対照的に, アテンションは, 値が特定の入力データに依存する重み係数を使用します。図 12.2 は, 自然言語でトレーニングされたトランスフォーマーネットワークのセクションからのアテンションの重みを示しています。
</p>
<center><img src="images/fig12_2.png"></center>
<p class="margin-large">
<center>図12.2 。学習されたアテンションの重みの例。 Vaswani ら 2017</center>
</p>
<p>
自然言語処理について説明する場合, 単語埋め込みを使用して単語を埋め込み空間内のベクトルにマッピングする方法がわかります。 これらのベクトルは, 後続のニューラル ネットワーク処理の入力として使用できます。 これらの埋め込みは, たとえば, 同様の意味を持つ単語を埋め込み空間内の近くの位置にマッピングすることによって, 基本的な意味論的特性を捕捉します。 このような埋め込みの特徴の1 つは, 特定の単語が常に同じ埋め込みベクトルにマップされることです。
</p>
<p>
トランスフォーマーは, 特定のベクトルがシーケンス内の他のベクトルに依存する位置にマッピングされる, よりリッチな形式の埋め込みとみなすことができます。したがって, 上記の例で bank を表すベクトルは, 2 つの異なる文の新しい埋め込み空間内の異なる場所にマッピングできます。たとえば, 最初の文では, 変換された表現により, 埋め込み空間内で bank が 水 に近くなる可能性がありますが, 2 番目の文では, 変換された表現により お金 に近くなる可能性があります。
</p>
<p>
アテンションの例として, タンパク質のモデリングを考えてみましょう。タンパク質は, アミノ酸と呼ばれる分子単位の一次元配列として見ることができます。タンパク質は潜在的に数百または数千のそのようなユニットを含むことができ, それぞれは22 の可能性のうちの1 つによって与えられます。生細胞では, タンパク質は3 次元構造に折りたたまれます。この構造では1 次元配列では大きく離れたアミノ酸が3 次元空間で物理的に近くなり, それによって相互作用することができます。トランスフォーマーモデルでは, これらの離れたアミノ酸が相互作用することができます。相互に 付随 することで, それらの3 次元構造をモデル化できる精度が大幅に向上します Vig et al., 2020 。
</p>
<h3>12.1.1. トランスフォーマー処理</h3>
<p>
トランスフォーマーへの入力データは, 次元 \(D\) の一連のベクトル \(\{\mathbf x_n\}\) です。ここで, \(n=1,..., N\) です。これらのデータベクトルをトークンと呼びます。ここで, トークンは, たとえば文内の単語, 画像内のパッチ, タンパク質内のアミノ酸などに対応します。トークンの要素 \(x_{n_i}\) は特徴と呼ばれます。 後で, 自然言語データおよび画像用のこれらのトークンベクトルを構築する方法を説明します。トランスフォーマーの強力な特性は, 異なるデータ型の混合を処理するために新しいニューラルネットワークアーキテクチャを設計する必要がなく, その代わりにデータ変数をトークンの結合セットに単純に結合できることです。
</p>
<p>
トランスフォーマーの動作を明確に理解する前に, 表記を正確にすることが重要です。標準的な規則に従って, データベクトルを次元 \(N×D\) の行列 \(X\) に結合します。この行列 \(X\) では, 図12.3 に図説するように \(n\) 番目の行がトークンベクトル \(\mathbf x_n^T\) で構成され \(n=1,...,N\) で行にラベルが付けられます。この行列は1 セットの入力トークンを表しており, ほとんどのアプリケーションでは, 各単語が1 つのトークンとして表されるテキストの独立したパッセージなど, 多くのトークンセットを含むデータセットが必要になることに注意してください。トランスフォーマーの基本的な構成要素は, データ行列を入力として受け取り, 出力と同じ次元の変換行列 \(X\) を作成する関数です。この関数は次の形式で書くことができます。
\[
\tilde{\mathbf X}=TransformaerLayer[\mathbf X] \tag{12.1}
\]
</p>
<p>
次に, 複数のトランスフォーマー層を連続して適用して, 豊富な内部表現を学習できる深いネットワークを構築できます。各トランスフォーマー層には独自の重みとバイアスが含まれており, この章で後ほど詳しく説明するように, 適切なコスト関数を使用した勾配降下法を使用して学習できます。
</p>
<center><img src="images/fig12_3.png"></center>
<p class="margin-large">
図12.3 。\(N×D\) 次元のデータ行列 \(X\) の構造。行 \(n\) は転置されたデータベクトル \(\mathbf x_n^T\) を表します。
</p>
<p>
単一のトランスフォーマー層自体は2 つのステージで構成されます。アテンションメカニズムを実装する第1 ステージでは, データ行列の列全体で異なるトークンベクトルからの対応する特徴を混合します。一方, 第2 ステージは各行に独立して作用し,
各トークンベクトル内の特徴を変換します。まず, アテンションのメカニズムを見てみましょう。
</p>
<h3>12.1.2 アテンション係数</h3>
<p>
埋め込み空間に入力トークンの集合 \(\mathbf x_1, \cdots, \mathbf x_N\) があり, これを同じ数のトークンを持つがより豊かな意味構造を捕らえる別の集合 \(\mathbf y_1, \cdots, \mathbf y_N\) にマッピングしたいとします。特定の出力ベクトル \(\mathbf y_n\) を考えてみましょう。\(\mathbf y_n\) の値は, 対応する入力ベクトル \(\mathbf x_n\) だけではなく, 集合内のすべてのベクトル \(\mathbf x_1, \cdots, \mathbf x_N\) にも依存する必要があります。 注意すると, この依存関係は, \(\mathbf y_n\) の修正された表現を決定するために特に重要な入力 \(\mathbf x_m\) に対してより強くなるはずです。これを実現する簡単な方法は, 各出力ベクトル \(\mathbf y_n\) を, 重み付け係数 \(a_{nm}\) を使用した入力ベクトル\(\mathbf x_1,\cdots, \mathbf x_N\) の線形結合になるように定義することです。
\[
\mathbf y_n=\sum_{m=1}^N a_{nm}\mathbf x_m \tag{12.2}
\]
ここで, \(a_{nm}\) はアテンション重みと呼ばれます。係数は, 出力 \(\mathbf y_n\) にほとんど影響を及ぼさない入力トークンではゼロに近く, 最も大きな影響を与える入力では最大になる必要があります。したがって, ある係数が大きく正になる一方で, 別の係数が大きく負になることで補うという状況を避けるために, 係数が負でないように制約します。また, 出力が特定の入力により多くの注意を払う場合, 他の入力への注意が低下することを犠牲にして, 係数の合計が1 になるように制約します。したがって,
重み付け係数は次の2 つの制約を満たす必要があります。
\[
\begin{align}
&a_nm \geq 0 \tag{12.3} \\
\\
&\sum_{m=1}^N a_{nm} =1 \tag{12.4}
\end{align}
\]
</p>
<p>
これらを総合すると, 各係数が \(0≦a_{nm}≦1\) の範囲内にあり, そのため係数が 1 の分割 を定義することを意味します。特殊なケース \(a_{mm}=1\) では, \(n≠m\) に対して \(a_{nm} = 0\) となり, したがって \(\mathbf y_m=\mathbf x_m\) となり, 入力ベクトルは変換によって変更されません。より一般的には, 出力 \(\mathbf y_m\) は, 一部の入力に他の入力よりも大きな重みが与えられた入力ベクトルのブレンドです。
</p>
<p>
各出力ベクトル \(\mathbf y_n\) に対して異なる係数のセットがあり, 制約 12.3 と 12.4 が \(n\) の値ごとに個別に適用されることに注意してください。これらの係数は入力データに依存しますが, その計算方法については後ほど説明します。
</p>
<h3>12.1.3 セルフアテンション</h3>
<p>
次の問題は, 係数 \(a_{nm}\) をどのように決定するかです。これについて詳しく説明する前に, まず情報検索の分野で使われている用語をいくつか紹介しておきます。オンライン映画ストリーミングサービスでどの映画を見るかを選択するという問題を考えてみましょう。1 つのアプローチは, 各映画を, ジャンル コメディ, アクションなど)), 主演俳優の名前, 映画の長さなどを説明する属性のリストに関連付けることです。ユーザーはカタログを検索して, 自分の好みに合った映画を見つけることができます。各ムービーの属性をキーと呼ばれるベクトルにエンコードすることで, これを自動化できます。対応する動画ファイル自体をバリューと呼びます。同様に, ユーザーは, 必要な属性に対して独自の値のベクトル クエリと呼ばれます を提供できます。その後, 映画サービスはクエリベクトルをすべてのキーベクトルと比較して最適な一致を見つけ, 対応する映画を値ファイルの形式でユーザーに送信できます。ユーザーが, キーがクエリに最も近い特定の映画に 参加している と考えることができます。これは, 単一の値ベクトルが返されるハードアテンションの一形態と考えられます。トランスフォーマーの場合, これをソフトアテンションに一般化し, 連続変数を使用してクエリとキー間の一致度を測定し, これらの変数を使用して出力に対する値ベクトルの影響を重み付けします。これにより, 変換関数が微分可能であることも保証され, 勾配降下法でトレーニングできるようになります。
</p>
<p>
情報検索との類似に従って, 各入力ベクトルxn を, 出力トークンの作成に使用される値ベクトルとして見ることができます。 また, ベクトルx n を入力トークンn のキー ベクトルとして直接使用します。それは, 映画そのものを使って映画の特徴を要約することに似ています。 最後に, \(\mathbf x_m\) を出力 \(\mathbf y_m\) のクエリ ベクトルとして使用し, 各キー ベクトルと比較できます。\(\mathbf x_n\) で表されるトークンが \(\mathbf x_m\) で表されるトークンにどの程度注意を払う必要があるかを確認するには, これらのベクトルがどの程度類似しているかを計算する必要があります。類似性の簡単な尺度の1 つは, それらの内積 \(\mathbf x_n^T\mathbf x_m\) を取得することです。制約12.3 と 12.4 を課すために, softmax 関数を使用してドット積を変換することにより, 重み付け係数 \(a_{nm}\) を定義できます。
\[
a_{nm}=\frac{exp(\mathbf x_n^T\mathbf x_m)}{\sum_{m^\prime=1}^N
 exp(\mathbf x_n^T\mathbf x_{m^\prime})} \tag{12.5}
\]
この場合, ソフトマックス関数の確率的な解釈はなく, 単にアテンションの重みを適切に正規化するために使用されていることに注意してください。
</p>
<p>
要約すると, 各入力ベクトル \(\mathbf x_n\) は, 12.2 の形式の入力ベクトルの線形結合を取ることによって, 対応する出力ベクトル \(\mathbf y_n\) に変換されます。入力ベクトル \(\mathbf x_m\) に適用される重み \(a_{nm}\) は, ソフトマックス関数 12.5 によって与えられます。入力 \(n\) のクエリ \(\mathbf x_n\) と入力 \(m\) に関連付けられたキー \(\mathbf x_m\) の間のドット積 \(\mathbf x_n^T\mathbf x_m\) で定義されます。すべての入力ベクトルが直交している場合, 各出力ベクトルは対応する入力ベクトルと単純に等しくなり \(m = 1, \cdots, N\) に対して \(\mathbf y_m = \mathbf x_m\) なることに注意してください。
</p>
<p>
データ行列 \(X\) と, 行が \(\mathbf y_m\) で与えられる類似の \(N×D\) 出力行列 \(Y\) を使用して 12.2 を行列表記で書くことができます。
\[
Y=Softmax[XX^T]X \tag{12.6}
\]
</p>
<p>
ここで, \(Softmax[L]\) は, 行列 \(L\) のすべての要素の指数を取得し, 各行を個別に正規化して合計が1 になる演算子です。ここからは, わかりやすくするために行列表記に焦点を当てていきます。
</p>
<p>
同じシーケンスを使用してクエリ, キー, 値を決定するため, このプロセスはセルフアテンションと呼ばれます。この注意メカニズムの変形については, この章の後半で説明します。また, クエリベクトルとキーベクトル間の類似性の尺度はドット積によって与えられるため, これはドット積セルフアテンションとして知られています。
</p>
<h3>12.1.4 ネットワーク・パラメータ</h3>
<p>
現状では, 入力ベクトル \(\{\mathbf x_n\}\) から出力ベクトル \(\{\mathbf y_n\}\) への変換は固定されており, 調整可能なパラメーターがないため, データから学習する能力がありません。さらに, トークンベクトル \(\mathbf x_n\) 内の各特徴値は, 注目係数を決定する際に同等の役割を果たしますが, ネットワークには, トークンの類似性を決定する際に, 他の特徴よりも一部の特徴に重点を置く柔軟性を持たせたいと考えています。元のベクトルの線形変換によって与えられる修正された特徴ベクトルを次の形式で定義すると, 両方の問題に対処できます。
\[
\tilde{X}=XU \tag{12.7}
\]
ここで, \(U\) は学習可能な重みパラメータの \(D×D\) 行列で, 標準的なニューラルネットワークの層に似ています。これにより, フォームの変更された変換が得られます。
\[
Y=Softmax[XUU^TX^T]XU \tag{12.8}
\]
</p>
<p>
これは柔軟性がはるかに優れていますが, マトリックスが次のような特性を持っています。
\[
XUU^TX^T \tag{12.8}
\]
これは対称ですが, アテンションメカニズムには重大な非対称性をサポートしてもらいたいと考えています。たとえば, すべてのノミは道具であるため, ノミ は ツール と強く関連付けられるはずですが, ノミ以外にも多くの種類のツールがあるため,
ツール は ノミ と弱く関連付けられるだけであると予想できます。ソフトマックス関数は, 結果として得られるアテンション重みの行列自体が対称ではないことを意味しますが, クエリとキーに独立したパラメーターを持たせることで, より柔軟なモデルを作成できます。さらに, 形式 12.8 は同じパラメータ行列U を使用して値ベクトルと注意係数の両方を定義しますが, これも望ましくない制限のように思えます。
</p>
<p>
これらの制限は, それぞれが独自の独立した線形変換を持つ個別のクエリ, キー, および値の行列を定義することで克服できます。
\[
\begin{align}
Q &= ZW^{(q)} \tag{12.10} \\
\\
K &= XW^{(k)} \tag{12.11} \\
\\
V &=XW^{(v)} \tag{12.12}
\end{align}
\]
ここで, 重み行列 \(W^{(q)}, W^{(k)}\), および \(W^{(v)}\) は, 最終的なトランスフォーマー・アーキテクチャのトレーニング中に学習されるパラメーターを表します。ここで, 行列 \(W^{(k)}\) の次元は \(D×D_k\) です。ここで, \(D_k\) はキー ベクトルの長さです。クエリベクトルとキーベクトルの間の内積を形成できるように, 行列 \(W^{(q)}\) は \(W^{(k)}\) と同じ次元 \(D×D_k\) を持たなければなりません。 一般的な選択は \(D_k = D\) です。同様に, \(W^{(v)}\) はサイズ \(D×D_v\) の行列で, \(D_v\) は出力ベクトルの次元を決定します。出力表現が入力と同じ次元になるように \(D_v = D\) を設定すると, 後で説明する残差接続の組み込みが容易になります。また, 各層の次元が同じであれば, 複数のトランスフォーマー層を相互に積み重ねることができます。次に, 12.6 を一般化して次のようにすることができます。
\[
Y=Softmax[QK^T]V \tag{12.13}
\]
ここで, \(QK^T\) の次元は \(N×N\) で, 行列 \(Y\) の次元は \(N×D_v\) です。行列 \(QK^T\) の計算を図12.4 に示し, 行列 \(Y\) の評価を図12.5 に示します。
</p>
<center><img src="images/fig12_4.png"></center>
<p class="margin-large">
図12.4 。トランスフォーマーのアテンション係数を決定する行列 \(QK^T\) の評価の図。入力 \(X\) は, 12.10 と 12.11 を使用して個別に変換され, それぞれクエリ行列 \(Q\) とキー行列 \(K\) が得られ, 次にそれらが乗算されます。
</p>
<center><img src="images/fig12_5.png"></center>
<p class="margin-large">
図12.5 。クエリ行列, キー行列, 値行列それぞれ \(Q, K, V\) を与えた場合のアテンション層からの出力の評価の図。出力行列Y で強調表示された位置のエントリは, \(Softmax[QK^T]\) 行列と \(V\) 行列の強調表示された行と列の内積からそれぞれ取得されます。
</p>
<p>
実際には, これらの線形変換にバイアスパラメーターを含めることもできます。ただし, 標準のニューラルネットワークで行ったように, データ行列X に1 の追加列を追加し, バイアスを表すパラメータの追加行で重み行列を拡張することにより, バイアスパラメータを重み行列に吸収することができます。今後は, 表記が煩雑になることを避けるために, バイアスパラメータを暗黙的なパラメータとして扱います。
</p>

<p>
従来のニューラルネットワークと比較して, 信号パスは活性化値間に乗算関係を持ちます。標準的なネットワークではアクティベーションに固定の重みを乗算しますが, ここではアクティベーションにデータ依存のアテンション係数を乗算します。これは, たとえば, 入力ベクトルの特定の選択に対して注目係数の1 つがゼロに近い場合, 結果の信号パスは対応する入力信号を無視するため, ネットワーク出力に影響を与えないことを意味します。対照的に, 標準的なニューラルネットワークが特定の入力または隠れユニット変数を無視することを学習すると, すべての入力ベクトルに対して無視します。
</p>
<h3>12.1.5 スケーリングされたセルフアテンション</h3>
<p>
セルフアテンション層に対して行うことができる最後の調整が1 つあります。ソフトマックス関数の勾配は, tanh 関数やロジスティック シグモイド活性化関数の場合と同様に, 入力の大きさが大きい場合には指数関数的に小さくなることを思い出してください。 これを防ぐには, ソフトマックス関数を適用する前に, クエリとキーベクトルの積を再スケーリングします。適切なスケーリングを導出するには, クエリベクトルとキーベクトルの要素がすべて, 平均と単位分散がゼロの独立した乱数である場合, 内積の分散は \(D_k\) になることに注意してください。したがって, \(D_k\) の平方根で与えられる標準偏差を使用して引数をソフトマックスに正規化し, 注目層の出力は次の形式になります。
\[
\mathbf Y=Attention(\mathbf Q,\mathbf K,\mathbf V)\equiv Softmax\left[\frac{\mathbf Q \mathbf K^T}{\sqrt{D_k}}\right]\mathbf V \tag{12.14}
\]
これはスケーリングされたドット積セルフ アテンションと呼ばれ, セルフアテンションニューラル ネットワーク層の最終形式です。この層の構造は, 図12.6 とアルゴリズム12.1 にまとめられています。
</p>
<p>
図12.6 。スケーリングされたドット積セルフアテンションニューラルネットワーク層の情報の流れ。ここで, mat mul は行列の乗算を示し, scale は \(D_k\) を使用したソフトマックスへの引数の正規化を示します。この構造は, 単一の注意 ヘッドを構成します。
</p>
<p>
\[
\boxed{
\begin{array}{l}
\textbf{アルゴリズム 12.1:　スケーリングされたドット積セルフアテンション} \\
\hline
入力 :トークンの集合 X\in \mathbb R^{N\times D}:\{\mathbf x_1,\cdots,\mathbf x_N\}\\
　　　重み行列 \{W^{(q)},W^{(k)}\}\in \mathbb R^{D\times D_k} と W^{(v)}\in \mathbb R^{D\times D_v}\\
出力：アテンション(Q,K,V)\in \mathbb R^{N\times D_v}:\{\mathbf y_1,\cdots,\mathbf y_N\} \\

\hline
Q=XW^{(q)}　// クエリ　Q\in\mathbb R^{N\times D_k}を計算する\\
K=XW^{(k)}　// キー　　K\in\mathbb R^{N\times D_k}を計算する\\
V=XW^{(v)}　// バリューV\in\mathbb R^{N\times D}を計算する\\
return　Attention(Q,K,V)=Softmax\left[\frac{QK^T}{\sqrt{D_k}}\right]V
\end{array}
}
\]
</p>
<h3>12.1.6 マルチヘッド・アテンション</h3>
<p>
これまで説明したアテンション層は, 出力ベクトルが入力ベクトルのデータ依存パターンに対応できるようにするものであり,
アテンションヘッドと呼ばれます。ただし, 同時に関連する注意のパターンが複数存在する可能性があります。たとえば, 自然言語では, 時制に関連するパターンもあれば, 語彙に関連するパターンもあります。単一のアテンションヘッドを使用すると, これらの効果が平均化される可能性があります。代わりに, 複数のアテンションヘッドを並行して使用できます。これらは, クエリ, キー, および値の行列の計算を制御する独立した学習可能なパラメーターを備えた, 単一ヘッドの同一構造のコピーで構成されます。これは, 畳み込みネットワークの各層で複数の異なるフィルターを使用することに似ています。
</p>
<p>
次の形式の \(h = 1,\cdots, H\) でインデックス付けされた \(H\) 個のヘッドがあるとします。
\[
H_h=Attention(Q_h,K_h,V_h) \tag{12.15}
\]
ここで, \(Attention( ･, ･, ･)\) は 12.14 で与えられ, 各ヘッドに対して個別のクエリ, キー, およびバリューの行列を次のように定義しました。
\[
\begin{align}
Q_h &= XW_h^{(q)} \tag{12.16} \\
\\
K_h &= XW_h^{(k)} \tag{12.17} \\
\\
V_h &= XW_h^{(v)} \tag{12.18}
\end{align}
\]
まずヘッドが1 つの行列に連結され, その結果が行列 \(W^{(o)}\) を使用して線形変換され, 次の形式で結合された出力が得られます。
\[
Y(X)=Concat[H_1,\cdots,H_H]W^{(o)} \tag{12.19}
\]
これを図 12.7 に示します。
</p>
<center><img src="images/fig12_7.png"></center>
<p>
図12.7 。マルチヘッドアテンションのためのネットワークアーキテクチャ。各ヘッドは図12.6 に示す構造で構成され, 独自のキー, クエリ, 値パラメータを持ちます。ヘッドの出力は連結され, 入力データの次元に線形投影されます。
</p>
<p>
各行列 \(H_h\) の次元は \(N×D_v\) であるため, 連結された行列の次元は\(N×HD_v\) になります。これは, 次元 \(HD_v×D\) の線形行列 \(W^{(o)}\) によって変換され, 元の入力行列 \(X\) と同じ次元 \(N×D\) の最終出力行列 \(Y\) が得られます。行列 \(W^{(o)}\) の要素 これらは, トレーニング フェーズ中にクエリ, キー, および値のマトリックスとともに学習されます。 通常, \(D_v\) は \(D/H\) に等しくなるように選択され, 結果の連結行列の次元が \(N×D\) になるようにします。マルチヘッド アテンションはアルゴリズム 12.2 にまとめられており, マルチヘッド アテンション層の情報フローは図 12.8 に示されています。
</p>
<p>
\[
\boxed{
\begin{array}{l}
\textbf{アルゴリズム 12.2:　マルチヘッド・アテンション} \\
\hline
入力 :トークンの集合 X\in \mathbb R^{N\times D}:\{\mathbf x_1,\cdots,\mathbf x_N\}\\
　　　クエリ重み行列 \{W_1^{(q)},\cdots,W_H^{(q)}\}\in \mathbb R^{D\times D}\\
　　　キー重み行列 \{W_1^{(k)},\cdots,W_H^{(k)}\}\in \mathbb R^{D\times D}\\
　　　バリュー重み行列 \{W_1^{(v)},\cdots,W_H^{(v)}\}\in \mathbb R^{D\times D}\\
　　　出力重み行列 W^{(o)}\in \mathbb R^{HD_v\times D}
 と W^{(v)}\in \mathbb R^{D\times D_v}\\
出力：Y \in \mathbb R^{N\times D}:\{\mathbf y_1,\cdots,\mathbf x_N\} \\
\hline
// 各ヘッドに対してセルフアテンションを計算する(アルゴリズム12.1)\\
for\;h=1,\cdots,H\;do\\
　|　Q_h ＝XW_h^{(q)},　K_h=XW_h^{(k)},　V_h=XW_h^{(v)} \\
　|　H_h = Attention(Q_h,K_h,V_h) // ヘッドを連結する \\
end\,for\\
H=Concat[H_1,\cdots,H_N] \\
return　Y(X)=HW^{(o)}
\end{array}
}
\]
</p>
<center><img src="images/fig12_8.png"></center>
<p>
<center>図12.8 。マルチヘッドアテンション層の情報の流れ</center>
</p>
<h3>12.1.7 トランスフォーマー層</h3>
<p>
マルチヘッドセルフアテンションは, トランスフォーマーネットワークの中核となるアーキテクチャ要素を形成します。ニューラルネットワークが深さから大きな恩恵を受けることはわかっているので, 複数のセルフアテンション層を互いに積み重ねたいと考えています。トレーニング効率を向上させるために, マルチヘッド構造をバイパスする残差接続を導入できます。これを行うには, 出力の次元が入力の次元と同じ, つまり \(N×D\) である必要があります。次に, 層の正規化 (Ba , Kiros , およびHinton , 2016) が続き, これによりトレーニングの効率が向上します。結果の変換は次のように記述できます。
\[
Z=LayerNorm[Y(X)+X] \tag{12.20}
\]
ここで, \(Y\) は (12.19) によって定義されます。場合によっては, 層の正規化が, より効果的な最適化をもたらす可能性があるため,
正規化層がマルチヘッドセルフセルフアテンションの後ではなく前に適用されるプレノームに置き換えられます。その場合, 次のようになります。
\[
Z=Y(X^\prime)+X　ここで　X^\prime=LayerNorm[X] \tag{12.21}
\]
いずれの場合も, \(Z\) は入力行列 \(X\) と同じ次元 \(N×D\) です。
</p>
<p>
アテンションメカニズムが値ベクトルの線形結合を作成し, それらが線形結合されて出力ベクトルが生成されることがわかりました。また, 値は入力ベクトルの線形関数であるため, アテンション層の出力は入力の線形結合になるように制約されていることがわかります。非線形性はアテンションウェイトを介して入り込むため, 出力はソフトマックス関数を介して入力に非線形に依存しますが, 出力ベクトルは依然として入力ベクトルが張る部分空間内に存在するように制約されており, これにより, アテンション層の表現力が制限されます。 多層パーセプトロン を意味する MLP[･] と呼ばれる, \(D\) 入力と \(D\) 出力を持つ標準的な非線形ニューラルネットワークを使用して各層の出力を後処理することで, トランスフォーマーの柔軟性を高めることができます。たとえば, これは, ReLU 隠れユニットを備えた 2 層の完全に接続されたネットワークで構成されている場合があります。これは, 可変長のシーケンスを処理するトランスフォーマーの能力を維持する方法で行う必要があります。これを達成するために, Z の行に対応する出力ベクトルのそれぞれに同じ共有ネットワークが適用されます。このニューラルネットワーク層も, 残差接続を使用することで改善できます。また, トランスフォーマー層からの最終出力が次の形式になるように層の正規化も含まれています。
\[
\tilde{X}=LayerNorm[MLP[Z]+Z] \tag{12.22}
\]
これにより, トランスフォーマー層の全体的なアーキテクチャが図12.9 に示され, アルゴリズム12.3 に要約されます。ここでも, 代わりにプレノルムを使用できます。その場合, 最終出力は次のようになります。
\[
\tilde{X}=MLP(Z^\prime)+Z　ここで　Z^\prime=LayerNorm[Z] \tag{12.23}
\]
</p>
<center><img src="images/fig12_9.png"></center>
<p>
図12.9　変換(12.1)を実装するトランスフォーマー アーキテクチャの1つの層 。 ここで, MLP は多層パーセプトロンを表し, "add and norm" は層の正規化が後に続く残差接続を示します。
</p>
<h3>12.1.8 計算複雑度</h3>
<p>
これまで説明したアテンション層は, それぞれ長さ \(D\) の \(N\) 個のベクトルの集合を取得し, それらを同じ次元を持つ別の \(N'\) 個のベクトルの集合にマッピングします。したがって, 入力と出力はそれぞれ全体の次元数 \(ND\) となります。標準の全結合ニューラルネットワークを使用して入力値を出力値にマッピングした場合, \(\mathcal O(N^2D^2)\) 個の独立したパラメーターを持つことになります。同様に, そのようなネットワークを介した1 つの順方向パスを評価する計算コストも \(\mathcal O(N^2D^2)\) になります。
</p>
<p>
アテンション層では, 行列 \(W^{(q)}, W^{(k)}, W^{(v)}\) が入力トークン間で共有されるため, \(D_k\simeq D_v\simeq D\) と仮定すると, 独立パラメータの数は\(\mathcal O(D^2)\) になります。\(N\) 個の入力トークンがあるため, セルフアテンション層の内積を評価する際の計算ステップ数は \(\mathcal O(N^2D)\) です。セルフアテンション層は, 行列の特定のブロック間でパラメータが共有される疎行列と考えることができます。
後続のニューラルネットワーク層には \(D\) 入力と \(D\) 出力があり, コストは \(\mathcal O(D^2)\) です。これはトークン間で共有されるため,  \(N\) で線形な複雑性があり, したがって全体としてこの層のコストは\(\mathcal O(ND^2)\) になります。\(N\)と \(D\) の相対的なサイズに応じて, トランスフォーマー層または MLP層のいずれかが計算コストを支配する可能性があります。全結合されたネットワークと比較して, トランスフォーマー層は計算効率が高くなります。トランスフォーマーアーキテクチャの多くの変形例が提案されています((Lin er al., 2021; Phuong and Hutter, 2022Lin er al., 2021; Phuong and Hutter, 2022))。これには, 効率の向上を目的とした修正も含まれます((Tay et al. 2020Tay et al. 2020))。
</p>
<h3>12.1.9 位置エンコーディング</h3>
<p>
トランスフォーマーアーキテクチャでは, 後続のニューラルネットワークと同様に, 行列Wh q )), Wh k )), およびWh v が入力トークン全体で共有されます。結果として, トランスフォーマには, 入力トークン, つまりX の行の順序を変更すると, 出力行列~X の行も同じ順序で変更されるという特性があります。言い換えれば, トランスフォーマーは入力順列に関して等変です。ネットワークアーキテクチャでのパラメータの共有により, トランスフォーマの大規模な並列処理が容易になり, ネットワークが長距離の依存関係を短距離の依存関係と同じくらい効果的に学習できるようになります。ただし, 自然言語の単語などの連続データを考慮する場合, トークンの順序に依存しないことが大きな制限になります。これは, トランスフォーマーによって学習される表現が入力トークンの順序から独立しているためです。 食べ物は悪く, まったく良くありませんでした。 と 食べ物はおいしかったですが, まったく悪くありませんでした。 という2 つの文 には同じトークンが含まれていますが, トークンの順序が異なるため, 意味が大きく異なります。明らかに, トークンの順序は, 自然言語処理を含むほとんどの逐次処理タスクにとって重要であるため, トークンの順序情報をネットワークに注入する方法を見つける必要があります。
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
    </body>
</html>